[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SLE-autoantibody-prediction",
    "section": "",
    "text": "This research project aims to predict diagnosis of Systemic Lupus Erythematosus using machine learning and microarray measurements of auto-antibodies . The (python) code is available on GitHub and belongs to the following publication:\nBrunekreef TE, Reteig LC, Limper M, Haitjema S, Dias J, Mathsson-Alm L, van Laar JM, Otten HG. Microarray analysis of autoantibodies can identify future Systemic Lupus Erythematosus patients. Human Immunology. 2022 Jun 1;83(6):509-14."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "SLE-autoantibody-prediction",
    "section": "Contents",
    "text": "Contents\nThe navigation bar on the left links to all the computational notebooks with the analyses and results for this project. The Main Results notebook contains all the results that are reported in the paper. The other notebooks contain all the (exploratory) analyses that we ran (for which we were able to publish the code).\nThere’s also a project package with a number of python modules containing supporting code that are imported in the notebooks.\nThe original patient data were not shared along with the publication. If you don’t have access to this data, but would still like to run the code, the notebooks offer the option to generate some simple synthetic data."
  },
  {
    "objectID": "index.html#reproducibility",
    "href": "index.html#reproducibility",
    "title": "SLE-autoantibody-prediction",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nYou’ll need either the conda or the mamba package manager to recreate the computational environment. It might take conda a while to resolve the environment in environment.yml (see step 3), so it’s recommended to use mamba instead.\nIf you already have conda installed, you can install mamba as follows:\nconda install mamba -n base -c conda-forge\nIf you don’t have conda, you can skip it and install mambaforge instead:\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\nbash Mambaforge-$(uname)-$(uname -m).sh\nClone this repo, e.g.:\ngit clone https://github.com/umcu/SLE-autoantibody-prediction\ncd SLE-autoantibody-prediction\nMake and activate the virtual environment, e.g.:\nmamba env create -f environment.yaml\nconda activate SLE\nInstall the project package, e.g.\npip install -e .\nOpen and run any of the notebooks, for instance with JupyterLab:\njupyter lab"
  },
  {
    "objectID": "notebooks/Nefritis Versus non-nefritis.html",
    "href": "notebooks/Nefritis Versus non-nefritis.html",
    "title": "Predict whether an SLE patient has nefritis or not",
    "section": "",
    "text": "import os\nimport pandas as pd\nimport numpy as np\nimport feather\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sle.modeling import generate_data, prep_data\nfrom sle.penalization import regularization_range, choose_C, regularization_path, plot_regularization_path, coef_plots_regularized\n%load_ext autoreload\n%autoreload 2\n\n\n\n\n\n\n\nRunning the code without the original data\n\n\n\nIf you want to run the code but don’t have access to the data, run the following instead to generate some synthetic data:\n\n\ndata_all = generate_data('imid')\nX_test_df = generate_data('rest')\n\n\nCode for loading original data\ndata_dir = os.path.join('..', 'data', 'processed')\ndata_all = feather.read_dataframe(os.path.join(data_dir, 'imid.feather'))\nX_test_df = feather.read_dataframe(os.path.join(data_dir,'rest.feather'))\n\n\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=40)\ntrf = PowerTransformer(method='box-cox')\n\n\ny_nefritis = data_all[data_all.Class=='SLE'].Nefritis.astype(np.int64)\nX_nefritis = data_all[data_all.Class=='SLE'].drop([\"Class\"]+[\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"], axis=1)\n\n\nXp1_nefritis = X_nefritis + 1 # Some < 0 values > -1. Because negative fluorescence isn't possible, and Box-Cox requires strictly positive values, add ofset\nX_trf_nefritis = pd.DataFrame(trf.fit_transform(Xp1_nefritis), index=X_nefritis.index, columns=X_nefritis.columns)\n\n\nclf_lasso = LogisticRegression(penalty='l1', max_iter = 10000, solver = 'liblinear')\n\n\nK = 100\nlambda_min, lambda_max = regularization_range(Xp1_nefritis,y_nefritis,trf)\nCs_lasso_nefritis = np.logspace(np.log10(1/lambda_min),np.log10(1/lambda_max), K)\npipe = Pipeline([\n        ('trf', trf),\n        ('clf', clf_lasso)\n])\nparams = [{\n    \"clf__C\": Cs_lasso_nefritis\n}]\n\nlasso_nefritis = GridSearchCV(pipe, params, cv = cv, scoring = 'roc_auc', refit=choose_C)\n\n\n%%time\nlasso_nefritis.fit(Xp1_nefritis,y_nefritis)\n\nCPU times: user 4min 53s, sys: 351 ms, total: 4min 53s\nWall time: 5min 49s\n\n\nGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=40),\n             estimator=Pipeline(steps=[('trf',\n                                        PowerTransformer(method='box-cox')),\n                                       ('clf',\n                                        LogisticRegression(max_iter=10000,\n                                                           penalty='l1',\n                                                           solver='liblinear'))]),\n             param_grid=[{'clf__C': array([28.71916258, 26.78358714, 24.97846301, 23.2949982 , 21.72499329,\n       20.26080145, 18.89529124, 17.62181185,...\n        0.21724993,  0.20260801,  0.18895291,  0.17621812,  0.16434161,\n        0.15326553,  0.14293595,  0.13330254,  0.1243184 ,  0.11593975,\n        0.1081258 ,  0.10083849,  0.09404231,  0.08770417,  0.08179321,\n        0.07628062,  0.07113956,  0.06634499,  0.06187356,  0.05770349,\n        0.05381447,  0.05018755,  0.04680508,  0.04365057,  0.04070867,\n        0.03796504,  0.03540633,  0.03302006,  0.03079462,  0.02871916])}],\n             refit=<function choose_C at 0x7fb3520bd560>, scoring='roc_auc')\n\n\nBest model:\n\nlasso_nefritis.cv_results_['mean_test_score'].max()\n\n0.6029809240238928\n\n\nScore with lambda selected through 1 SE rule:\n\nlasso_nefritis.cv_results_['mean_test_score'][lasso_nefritis.best_index_]\n\n0.5939512492715618\n\n\n\ncoefs_lasso_nefritis, nnz_coefs_lasso_nefritis = regularization_path(Cs_lasso_nefritis, clf_lasso, X_trf_nefritis, y_nefritis)\n\n\nax1, ax2, ax22 = plot_regularization_path(1/Cs_lasso_nefritis, coefs_lasso_nefritis, nnz_coefs_lasso_nefritis, lasso_nefritis.cv_results_)\n#ax22.set_ylim([0.8, 1])\n\n\n\n\n\ncoef_plots_regularized(coefs_lasso_nefritis, nnz_coefs_lasso_nefritis, lasso_nefritis.cv_results_[\"mean_test_score\"], varnames=Xp1_nefritis.columns)\n\n\n\n\nNon-zero coefficients of the model selected with the 1SE rule:\n\n(pd.Series(lasso_nefritis.best_estimator_.named_steps.clf.coef_.squeeze(), index = X_nefritis.columns)[lambda x: x!=0].sort_values(ascending=False))\n\nC1q            0.177918\nGAPDH          0.157594\ndsDNA2         0.116051\nCMV            0.110894\nMi2            0.041954\nASCA           0.030033\nNucleosome     0.005160\nSMP            0.003712\nCollagenII    -0.001279\nEnolasearg    -0.009766\nTPO           -0.046097\nCardiolipin   -0.068313\nGBM           -0.112145\nRNAPolIII     -0.144926\ndtype: float64"
  },
  {
    "objectID": "notebooks/preprocess.html",
    "href": "notebooks/preprocess.html",
    "title": "Preprocessing",
    "section": "",
    "text": "Citation\n\n\n\nThis notebook contains code for the following project:\nBrunekreef TE, Reteig LC, Limper M, Haitjema S, Dias J, Mathsson-Alm L, van Laar JM, Otten HG. Microarray analysis of autoantibodies can identify future Systemic Lupus Erythematosus patients. Human Immunology. 2022 Apr 11. doi:10.1016/j.humimm.2022.03.010"
  },
  {
    "objectID": "notebooks/preprocess.html#load-data",
    "href": "notebooks/preprocess.html#load-data",
    "title": "Preprocessing",
    "section": "Load data",
    "text": "Load data\n\nread_dir = os.path.join(\"..\", \"data\",\"interim\")\nwrite_dir = os.path.join(\"..\", \"data\",\"processed\")\nref_dir = os.path.join(\"..\", \"references\")\n\n\nother_imid_smps = pd.read_spss(os.path.join(read_dir, \"OtherIMID.sav\"),usecols=['samplenr']) # load only sample numbers from this file, as something's wrong with the formatting\nall_first = pd.read_spss(os.path.join(read_dir, \"Alles 1e sample.sav\")) # SLE patients (also has non- and other-IMID data)\nnon_imid = pd.read_spss(os.path.join(read_dir, \"Non-Imid control set.sav\"))\ntmo = pd.read_csv(os.path.join(read_dir, \"TMO.csv\")) # blood bank controls (also has data from SLE patients)\nrest_smps = pd.read_excel(os.path.join(read_dir, \"Restgroep voor vergelijking.xlsx\"), engine='openpyxl', usecols=['samplenr']) # contains sample numbers from rest group\n\n# load translation table of columns in TMO.csv vs. the .sav files\ndf_cols = pd.read_csv(os.path.join(ref_dir, \"chip_colnames.csv\"), sep=\";\")\n\n\nall_first = all_first.set_index('samplenr')\nnon_imid = non_imid.set_index('samplenr')\n\nThe all_first and tmo datasets contain data from more than one group\n\nsle = all_first[all_first.SLE == 1] # subset with only SLE patients\nblood_bank = tmo[tmo.Class == \"nonSLE\"] # subset with only blood bank controls\n\n\n# fix other-imids\nsmps_fixed = [f'{_:04}A' for _ in pd.to_numeric(other_imid_smps['samplenr'],downcast='unsigned')] # fix other_imid sample numbers\nother_imid = all_first.loc[smps_fixed] # get other-IMIDs from the full dataset\n\n\n# samples from patients who had no diagnosis at the time, but were later diagnosed with SLE\npre_smps = ['0039A','0159A','0222A','0228A','0575A','0633A','1080A','1117A','1158A','1160A','1166A','1193A','1223A','1305A','1451A','1981A','0972A']\npre_sle = all_first.loc[pre_smps]\n\n\n# samples from patients who had no diagnosis at the time, or later, and are not in the non-imid group\nrest_set = set(rest_smps['samplenr']) - set(non_imid.index)\nrest = all_first.loc[rest_set]\n\n\n# we can also define the rest group less strictly, as \"all the remaining samples\":\nrest_set_large = (set(all_first.index) - # set with all patient data from their first samples\n            set(non_imid.index) - # take out the non-IMIDS\n            set(other_imid.index) - # take out the other-IMIDS\n            set(sle.index) - # take out the SLE patients\n            set(all_first.index[all_first.dsDNA2.isna()]) - # take out those patients that weren't run on the chip (dsDNA2 column is empty)\n            set(pre_sle.index)) # take out the pre-SLE samples, because we want to compare them to this group\nrest_large = all_first.loc[rest_set_large]\n\n\n# samples from patients with lupus-like disease. N.B this includes 2 pre-sle patients\nlld = other_imid[other_imid.LLD==1]\n\n\nlen(set(all_first.index) - # set with all patient data from their first samples\n            set(non_imid.index) - # take out the non-IMIDS\n            set(other_imid.index) - # take out the other-IMIDS\n            set(sle.index) - # take out the SLE patients\n            set(all_first.index[all_first.dsDNA2.isna()]))\n\n472"
  },
  {
    "objectID": "notebooks/preprocess.html#process",
    "href": "notebooks/preprocess.html#process",
    "title": "Preprocessing",
    "section": "Process",
    "text": "Process\nThe chip columns are called differently in the blood_bank dataset than in the others.\n\ndf_cols\n\n\n\n\n\n  \n    \n      \n      TF\n      TB_all\n      TB_selection\n    \n  \n  \n    \n      0\n      Actinin\n      Actinin\n      Actinin\n    \n    \n      1\n      anti-IgE\n      antiIgE\n      NaN\n    \n    \n      2\n      ASCA\n      ASCA\n      ASCA\n    \n    \n      3\n      Beta2GP1\n      Beta2GP1\n      Beta2GP1\n    \n    \n      4\n      C1q\n      C1q\n      C1q\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      96\n      NaN\n      Strep15\n      NaN\n    \n    \n      97\n      NaN\n      Strep16\n      NaN\n    \n    \n      98\n      TIF1gamma\n      TIF1gamma\n      TIF1gamma\n    \n    \n      99\n      TPO\n      TPO\n      TPO\n    \n    \n      100\n      tTG\n      tTG\n      tTG\n    \n  \n\n101 rows × 3 columns\n\n\n\n\nTF are the names in blood_bank\nTB_all are the names in the other dfs\nTB_selection are names of the variables that should be most interesting (e.g. excluding control spots on the chip).\n\nEach row corresponds to the same variable, but it might have a different name in each column!\nRename the columns in blood_bank as in the other data sets:\n\nnew_colnames = df_cols.TB_all[df_cols.TF.notnull()].tolist() # list of new names for blood bank columns\nblood_bank = blood_bank.drop(columns='Class') # this column is in blood_bank, but not in the list (we'll add it back later)\nblood_bank.columns = new_colnames # rename columns as in other datasets\n\nWe want only the rows that have an entry in all three columns: these are the variables we want to use\n\nkeep_cols = df_cols.dropna().TB_all.tolist() # names of variables that exist in both datasets, and that are of interest\nkeep_cols\n\n['Actinin',\n 'ASCA',\n 'Beta2GP1',\n 'C1q',\n 'C3b',\n 'Cardiolipin',\n 'CCP1arg',\n 'CCP1cit',\n 'CENP',\n 'CMV',\n 'CollagenII',\n 'CpGmot',\n 'CRP1',\n 'DFS70',\n 'dsDNA2',\n 'Enolasearg',\n 'Enolasecit',\n 'EphB2',\n 'FcER',\n 'Fibrillarin',\n 'Ficolin',\n 'GAPDH',\n 'GBM',\n 'H2Bp',\n 'H2Bpac',\n 'H4p',\n 'H4pac',\n 'Histones',\n 'IFNLambda',\n 'IFNOmega',\n 'Jo1',\n 'Ku',\n 'LaSSB',\n 'MBL2',\n 'Mi2',\n 'Nucleosome',\n 'PCNA',\n 'Pentraxin3',\n 'PmScl100',\n 'RA33',\n 'RipP0',\n 'RipP0peptide',\n 'RipP1',\n 'RipP2',\n 'RNAPolIII',\n 'RNP70',\n 'RNPA',\n 'RNPC',\n 'Ro52',\n 'Ro60',\n 'RPP25ThTo',\n 'Scl70',\n 'SmBB',\n 'SMP',\n 'TIF1gamma',\n 'TPO',\n 'tTG']\n\n\nAside from diagnosis, we’re also interested in discrimating between patients depending on the presence of these 4 symptoms:\n\nsymptoms = ['Arthritis', 'Pleurisy', 'Pericarditis', 'Nefritis']\n\nLikewise, we also want to compare anti-dsDNA measured on the microchip array (dsDNA2) to the standard measurements taken in the clinic (dsDNA1)\n\nlab = ['dsDNA1']\n\nIn all datasets, keep only columns of interest\n\nblood_bank = blood_bank.loc[:,keep_cols] # we don't have symptoms info or lab dsDNA for this group\nother_imid = other_imid.loc[:,keep_cols+symptoms+lab] \nnon_imid = non_imid.loc[:,keep_cols+symptoms+lab]\nsle = sle.loc[:,keep_cols+symptoms+lab]\n\n\npre_sle = pre_sle.loc[:,keep_cols+symptoms+lab]\nrest = rest.loc[:,keep_cols+symptoms+lab]\nrest_large = rest_large.loc[:,keep_cols+symptoms+lab]\nlld = lld.loc[:,keep_cols+symptoms+lab]\n\nDiscard one SLE patient with missing data\n\nsle = sle.dropna(subset=keep_cols) # serum from one SLE patient was not run on chip\n\nAnd row-bind all the data frames together\n\n# add class to distinguish from others\nblood_bank['Class'] = \"BBD\"\nother_imid['Class'] = \"IMID\" \nnon_imid['Class'] = \"nonIMID\" \nsle['Class'] = \"SLE\"\n# join all data frames together by binding rows\ndf_all = pd.concat([sle, other_imid, non_imid, blood_bank])\n\n\nrest['Class'] = \"rest\"\nrest_large['Class'] = \"rest_large\"\npre_sle['Class'] = \"preSLE\"\nlld['Class'] = \"LLD\"\ndf_eval = pd.concat([pre_sle, rest, rest_large, lld])\n\n\ndf_all['Class'].value_counts()\n\nSLE        483\nBBD        361\nIMID       346\nnonIMID    218\nName: Class, dtype: int64\n\n\n\ndf_eval['Class'].value_counts()\n\nrest_large    462\nrest          415\nLLD            28\npreSLE         17\nName: Class, dtype: int64"
  },
  {
    "objectID": "notebooks/preprocess.html#write-data",
    "href": "notebooks/preprocess.html#write-data",
    "title": "Preprocessing",
    "section": "Write data",
    "text": "Write data\n\nfeather.write_dataframe(df_all, os.path.join(write_dir, \"imid.feather\"))\nfeather.write_dataframe(df_eval, os.path.join(write_dir, \"rest.feather\"))"
  },
  {
    "objectID": "notebooks/SLE Versus BBD.html",
    "href": "notebooks/SLE Versus BBD.html",
    "title": "Models for discriminating SLE patients from healthy controls (Blood Bank Donors)",
    "section": "",
    "text": "Citation\n\n\n\nThis notebook contains analyses for the following project:\nBrunekreef TE, Reteig LC, Limper M, Haitjema S, Dias J, Mathsson-Alm L, van Laar JM, Otten HG. Microarray analysis of autoantibodies can identify future Systemic Lupus Erythematosus patients. Human Immunology. 2022 Apr 11. doi:10.1016/j.humimm.2022.03.010"
  },
  {
    "objectID": "notebooks/SLE Versus BBD.html#setup",
    "href": "notebooks/SLE Versus BBD.html#setup",
    "title": "Models for discriminating SLE patients from healthy controls (Blood Bank Donors)",
    "section": "Setup",
    "text": "Setup\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport feather\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom stability_selection import StabilitySelection, plot_stability_path\nfrom joblib import parallel_backend\n\nfrom sle.regression import coef_from_sm, coef_plot, make_coef_tbl\nfrom sle.modeling import generate_data, prep_data, eval_model, calc_roc_cv, plot_roc_cv\nfrom sle.penalization import regularization_range, choose_C, regularization_path, plot_regularization_path, coef_plots_regularized\n%load_ext autoreload\n%autoreload 2\n\n/home/lcreteig/miniconda3/envs/SLE/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.linear_model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n  warnings.warn(message, FutureWarning)\n\n\n\n\n\n\n\n\nRunning the code without the original data\n\n\n\nIf you want to run the code but don’t have access to the data, run the following instead to generate some synthetic data:\n\n\ndata_all = generate_data('imid')\nX_test_df = generate_data('rest')\n\n\nCode for loading original data\ndata_dir = os.path.join('..', 'data', 'processed')\ndata_all = feather.read_dataframe(os.path.join(data_dir, 'imid.feather'))\nX_test_df = feather.read_dataframe(os.path.join(data_dir,'rest.feather'))\n\n\n\nX, y = prep_data(data_all, 'SLE', 'BBD', drop_cols = [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\ndsDNA = X.dsDNA2.values.reshape(-1,1) # only dsDNA from chip as a vector\n\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=40)\n\nDummy classifier\nAccuracy for dummy classifier:\n\ndummy_clf = DummyClassifier(random_state = 40, strategy = 'most_frequent')\ndummy_clf.fit(X, y)\ndummy_clf.predict(X),\ndummy_clf.score(X,y)\n\n0.5722748815165877"
  },
  {
    "objectID": "notebooks/SLE Versus BBD.html#logistic-regression-only-dsdna",
    "href": "notebooks/SLE Versus BBD.html#logistic-regression-only-dsdna",
    "title": "Models for discriminating SLE patients from healthy controls (Blood Bank Donors)",
    "section": "Logistic regression: Only dsDNA",
    "text": "Logistic regression: Only dsDNA\n\nclf = LogisticRegression(penalty = 'none', max_iter = 10000) # increase iterations for solver to converge\n\n\nlr_dsDNA = clf.fit(dsDNA, y)\n\n\nnp.mean(cross_val_score(clf, dsDNA, y, cv=cv, scoring = 'roc_auc'))\n\n0.7956906204723647\n\n\n\nlogreg_dsDNA = sm.Logit(y,sm.add_constant(dsDNA)).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.511833\n         Iterations 12\n\n\n\nnp.exp(coef_from_sm(logreg_dsDNA))\n\n\n\n\n\n  \n    \n      \n      ci_lower\n      ci_upper\n      beta\n    \n  \n  \n    \n      x1\n      1.015272\n      1.024399\n      1.019825\n    \n  \n\n\n\n\n\nTest on other groups\n\npre-SLE (vs. rest)LLD (vs. rest)IMID (vs. nonIMID)\n\n\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lr_dsDNA, X_test.dsDNA2.values.reshape(-1,1), y_test, 'preSLE', 'rest_large')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n  rest_large       0.98      0.79      0.87       462\n      preSLE       0.10      0.65      0.17        17\n\n    accuracy                           0.78       479\n   macro avg       0.54      0.72      0.52       479\nweighted avg       0.95      0.78      0.85       479\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. preSLE); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. preSLE); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\n\n\n\nX_test, y_test = prep_data(X_test_df, 'LLD', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lr_dsDNA, X_test.dsDNA2.values.reshape(-1,1), y_test, 'LLD', 'rest_large')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n  rest_large       0.96      0.79      0.86       462\n         LLD       0.12      0.46      0.19        28\n\n    accuracy                           0.77       490\n   macro avg       0.54      0.62      0.52       490\nweighted avg       0.91      0.77      0.83       490\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. LLD); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. LLD); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\n\n\n\nX_test, y_test = prep_data(data_all, 'IMID', 'nonIMID', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lr_dsDNA, X_test.dsDNA2.values.reshape(-1,1), y_test, 'IMID', 'nonIMID')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n     nonIMID       0.42      0.77      0.55       218\n        IMID       0.70      0.34      0.46       346\n\n    accuracy                           0.51       564\n   macro avg       0.56      0.55      0.50       564\nweighted avg       0.59      0.51      0.49       564\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. IMID); specificity for the other group (nonIMID)\nN.B.: \"precision\" = PPV for the group in this row (e.g. IMID); NPV for the other group (nonIMID)"
  },
  {
    "objectID": "notebooks/SLE Versus BBD.html#logistic-regression-without-dsdna",
    "href": "notebooks/SLE Versus BBD.html#logistic-regression-without-dsdna",
    "title": "Models for discriminating SLE patients from healthy controls (Blood Bank Donors)",
    "section": "Logistic regression without dsDNA",
    "text": "Logistic regression without dsDNA\n\nno_dsDNA = X.drop(columns='dsDNA2')\n\n\nnp.mean(cross_val_score(clf, no_dsDNA, y, cv=cv, scoring = 'roc_auc'))\n\n0.9007552888337073\n\n\n\nlogreg_no_dsDNA = sm.Logit(y,sm.add_constant(no_dsDNA)).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.258118\n         Iterations 11\n\n\n\ncoefs_no_dsDNA = coef_from_sm(logreg_no_dsDNA)\n\n\ncoef_plot(coefs_no_dsDNA, OR=False)\n\n\n\n\nWhen dropping dsDNA, CpGmot switches sign, and is now just barely significant (see below)"
  },
  {
    "objectID": "notebooks/SLE Versus BBD.html#logistic-regression-with-all-features",
    "href": "notebooks/SLE Versus BBD.html#logistic-regression-with-all-features",
    "title": "Models for discriminating SLE patients from healthy controls (Blood Bank Donors)",
    "section": "Logistic regression with all features",
    "text": "Logistic regression with all features\nVariance inflation factors\n\ndf_cor = X.corr()\nvifs = pd.Series(np.linalg.inv(df_cor.values).diagonal(), index=df_cor.index)\n\n\nvifs[vifs > 2].sort_values(ascending=False)\n\nRipP2           58.668002\nRipP1           21.379985\nRipP0           19.101134\nEnolasearg      11.417301\nCCP1arg          7.781867\nSmBB             7.486631\nSMP              6.739662\nEphB2            5.293658\nEnolasecit       5.076042\nRipP0peptide     4.229254\nRo60             4.180351\nRo52             4.064606\nNucleosome       3.890488\nRNPA             3.814536\nH2Bp             3.617355\nH2Bpac           3.566022\nH4p              3.216518\ndsDNA2           3.054496\nRNP70            3.052853\nRNPC             2.869649\nCpGmot           2.847054\nH4pac            2.793631\nHistones         2.727817\nCCP1cit          2.692775\ndtype: float64\n\n\n\nplt.figure(figsize=(13,13))\nsns.heatmap(df_cor, vmin=0.75, square = True)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fa2d6eb4810>\n\n\n\n\n\nWork down the list of VIFs. Whenever a correlation exceeds threshold, discard feature with highest VIF:\n\n# Set without correlations above .9\nX_nocor90 = X.drop(columns = [\n    'RipP2', # with RipP1 and RipP0\n    'Enolasearg' # with CCP1arg\n])\n\n\n# Set without correlations above .85\nX_nocor85 = X.drop(columns = [\n    'RipP2', # with RipP1 and RipP0\n    'Enolasearg', # with CCP1arg, EphB2\n    'SmBB' # with SMP\n])\n\n\n# Set without correlations above .8\nX_nocor80 = X.drop(columns = [\n    'RipP2', 'RipP1', # with RipP0\n    'Enolasearg', # with CCP1arg, EphB2, Enolasecit, RipP0peptide\n    'CCP1arg', # with EphB2\n    'SmBB', # with SMP\n    'Ro60', # with Ro52\n    'H2Bp' # with H2Bpac\n])\n\n\nWithout scaling:\n\nnp.mean(cross_val_score(clf, X, y, cv=cv, scoring = 'roc_auc'))\n\n0.9111973993393901\n\n\nRepeat with statsmodels, to get coefs / odds ratios and CIs:\n\nlogreg = sm.Logit(y,sm.add_constant(X)).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.234249\n         Iterations 13\n\n\n\nlogreg.summary().tables[0]\n\n\n\nLogit Regression Results\n\n  Dep. Variable:         Class        No. Observations:        844  \n\n\n  Model:                 Logit        Df Residuals:            786  \n\n\n  Method:                 MLE         Df Model:                 57  \n\n\n  Date:            Fri, 20 May 2022   Pseudo R-squ.:        0.6569  \n\n\n  Time:                10:33:04       Log-Likelihood:       -197.71 \n\n\n  converged:             True         LL-Null:              -576.17 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        6.396e-123\n\n\n\n\n\npd.read_html(logreg.summary().tables[1].as_html(), header=0, index_col=0)[0]\n\n\n\n\n\n  \n    \n      \n      coef\n      std err\n      z\n      P>|z|\n      [0.025\n      0.975]\n    \n  \n  \n    \n      const\n      -1.517100\n      0.462000\n      -3.287\n      0.001\n      -2.422000\n      -0.612000\n    \n    \n      Actinin\n      0.000062\n      0.001000\n      0.123\n      0.902\n      -0.001000\n      0.001000\n    \n    \n      ASCA\n      0.000011\n      0.000058\n      0.182\n      0.856\n      -0.000000\n      0.000000\n    \n    \n      Beta2GP1\n      0.000028\n      0.000053\n      0.535\n      0.593\n      -0.000075\n      0.000000\n    \n    \n      C1q\n      -0.000300\n      0.001000\n      -0.294\n      0.769\n      -0.002000\n      0.001000\n    \n    \n      C3b\n      -0.001900\n      0.001000\n      -1.315\n      0.189\n      -0.005000\n      0.001000\n    \n    \n      Cardiolipin\n      0.000200\n      0.000000\n      0.753\n      0.452\n      -0.000000\n      0.001000\n    \n    \n      CCP1arg\n      -0.001100\n      0.001000\n      -1.256\n      0.209\n      -0.003000\n      0.001000\n    \n    \n      CCP1cit\n      0.001700\n      0.001000\n      1.329\n      0.184\n      -0.001000\n      0.004000\n    \n    \n      CENP\n      0.000039\n      0.000054\n      0.720\n      0.472\n      -0.000067\n      0.000000\n    \n    \n      CMV\n      0.000200\n      0.000059\n      2.743\n      0.006\n      0.000046\n      0.000000\n    \n    \n      CollagenII\n      -0.002400\n      0.001000\n      -1.674\n      0.094\n      -0.005000\n      0.000000\n    \n    \n      CpGmot\n      -0.004600\n      0.003000\n      -1.383\n      0.167\n      -0.011000\n      0.002000\n    \n    \n      CRP1\n      0.001100\n      0.002000\n      0.635\n      0.526\n      -0.002000\n      0.004000\n    \n    \n      DFS70\n      -0.000019\n      0.000000\n      -0.168\n      0.867\n      -0.000000\n      0.000000\n    \n    \n      dsDNA2\n      0.012600\n      0.003000\n      4.164\n      0.000\n      0.007000\n      0.019000\n    \n    \n      Enolasearg\n      -0.000300\n      0.001000\n      -0.356\n      0.722\n      -0.002000\n      0.002000\n    \n    \n      Enolasecit\n      -0.000800\n      0.001000\n      -1.437\n      0.151\n      -0.002000\n      0.000000\n    \n    \n      EphB2\n      -0.002600\n      0.001000\n      -1.900\n      0.057\n      -0.005000\n      0.000084\n    \n    \n      FcER\n      -0.000600\n      0.001000\n      -0.626\n      0.532\n      -0.003000\n      0.001000\n    \n    \n      Fibrillarin\n      -0.000200\n      0.000000\n      -0.943\n      0.346\n      -0.001000\n      0.000000\n    \n    \n      Ficolin\n      -0.000500\n      0.002000\n      -0.301\n      0.764\n      -0.003000\n      0.003000\n    \n    \n      GAPDH\n      -0.000085\n      0.000000\n      -0.629\n      0.529\n      -0.000000\n      0.000000\n    \n    \n      GBM\n      -0.001800\n      0.001000\n      -2.750\n      0.006\n      -0.003000\n      -0.001000\n    \n    \n      H2Bp\n      -0.000100\n      0.000000\n      -0.292\n      0.770\n      -0.001000\n      0.001000\n    \n    \n      H2Bpac\n      0.000400\n      0.000000\n      0.761\n      0.447\n      -0.001000\n      0.001000\n    \n    \n      H4p\n      -0.000029\n      0.000000\n      -0.099\n      0.921\n      -0.001000\n      0.001000\n    \n    \n      H4pac\n      0.000047\n      0.000000\n      0.230\n      0.818\n      -0.000000\n      0.000000\n    \n    \n      Histones\n      -0.003600\n      0.003000\n      -1.304\n      0.192\n      -0.009000\n      0.002000\n    \n    \n      IFNLambda\n      0.000100\n      0.000000\n      0.848\n      0.397\n      -0.000000\n      0.000000\n    \n    \n      IFNOmega\n      -0.000100\n      0.000000\n      -0.271\n      0.787\n      -0.001000\n      0.001000\n    \n    \n      Jo1\n      0.002500\n      0.001000\n      2.114\n      0.034\n      0.000000\n      0.005000\n    \n    \n      Ku\n      0.001000\n      0.000000\n      1.984\n      0.047\n      0.000012\n      0.002000\n    \n    \n      LaSSB\n      0.000900\n      0.000000\n      2.652\n      0.008\n      0.000000\n      0.001000\n    \n    \n      MBL2\n      -0.000032\n      0.001000\n      -0.061\n      0.951\n      -0.001000\n      0.001000\n    \n    \n      Mi2\n      0.000400\n      0.000000\n      0.811\n      0.417\n      -0.001000\n      0.001000\n    \n    \n      Nucleosome\n      -0.000200\n      0.001000\n      -0.197\n      0.844\n      -0.002000\n      0.001000\n    \n    \n      PCNA\n      -0.002100\n      0.002000\n      -1.037\n      0.300\n      -0.006000\n      0.002000\n    \n    \n      Pentraxin3\n      0.000800\n      0.000000\n      2.115\n      0.034\n      0.000060\n      0.002000\n    \n    \n      PmScl100\n      0.000090\n      0.000065\n      1.388\n      0.165\n      -0.000037\n      0.000000\n    \n    \n      RA33\n      0.000200\n      0.000000\n      0.572\n      0.567\n      -0.001000\n      0.001000\n    \n    \n      RipP0\n      0.000500\n      0.002000\n      0.270\n      0.787\n      -0.003000\n      0.004000\n    \n    \n      RipP0peptide\n      -0.000600\n      0.000000\n      -1.935\n      0.053\n      -0.001000\n      0.000008\n    \n    \n      RipP1\n      0.000300\n      0.001000\n      0.265\n      0.791\n      -0.002000\n      0.002000\n    \n    \n      RipP2\n      0.002300\n      0.001000\n      1.840\n      0.066\n      -0.000000\n      0.005000\n    \n    \n      RNAPolIII\n      -0.000300\n      0.000000\n      -1.691\n      0.091\n      -0.001000\n      0.000051\n    \n    \n      RNP70\n      0.000100\n      0.000000\n      0.328\n      0.743\n      -0.001000\n      0.001000\n    \n    \n      RNPA\n      0.000400\n      0.000000\n      2.512\n      0.012\n      0.000088\n      0.001000\n    \n    \n      RNPC\n      -0.000400\n      0.000000\n      -1.522\n      0.128\n      -0.001000\n      0.000000\n    \n    \n      Ro52\n      0.000007\n      0.000071\n      0.094\n      0.925\n      -0.000000\n      0.000000\n    \n    \n      Ro60\n      0.000600\n      0.000000\n      4.675\n      0.000\n      0.000000\n      0.001000\n    \n    \n      RPP25ThTo\n      -0.001600\n      0.000000\n      -3.962\n      0.000\n      -0.002000\n      -0.001000\n    \n    \n      Scl70\n      -0.000300\n      0.000000\n      -1.295\n      0.195\n      -0.001000\n      0.000000\n    \n    \n      SmBB\n      0.000300\n      0.000000\n      1.679\n      0.093\n      -0.000049\n      0.001000\n    \n    \n      SMP\n      0.000300\n      0.000000\n      0.937\n      0.349\n      -0.000000\n      0.001000\n    \n    \n      TIF1gamma\n      0.002100\n      0.001000\n      2.663\n      0.008\n      0.001000\n      0.004000\n    \n    \n      TPO\n      0.000014\n      0.000018\n      0.790\n      0.430\n      -0.000021\n      0.000049\n    \n    \n      tTG\n      -0.001500\n      0.001000\n      -2.590\n      0.010\n      -0.003000\n      -0.000000\n    \n  \n\n\n\n\nSorted coefficients with CIs:\n\ncoefs = coef_from_sm(logreg)\n\nPlot Odds ratios:\n\ncoef_plot(np.exp(coefs), OR=True)\n\n\n\n\n\n\nWith standardization:\n\npipe_sc = Pipeline([\n        ('scale', RobustScaler()), # try RobustScaler because the data is highly skewed and have outliers\n        ('clf', clf)])\n\n\nnp.mean(cross_val_score(pipe_sc, X, y, cv=cv, scoring = 'roc_auc'))\n\n0.9117359345823722\n\n\n\nscale=RobustScaler()\nX_sc = pd.DataFrame(scale.fit_transform(X), index=X.index, columns=X.columns)\n\n\nlogreg_sc = sm.Logit(y,sm.add_constant(X_sc)).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.234249\n         Iterations 13\n\n\n\ncoefs_sc = coef_from_sm(logreg_sc)\n\n\ncoef_plot(coefs_sc, OR=False) # plotting ORs here is not feasible as Ro60 is so large\n\n\n\n\nThis makes Ro60 the highest-weighted feature, cf. XGBoost\n\n\nWith standardization + Box-Cox:\n\nXp1 = X + 1 # Some values are between -1 and 0. Because negative fluorescence isn't possible, and Box-Cox requires strictly positive values, add ofset\n\n\ntrf = PowerTransformer(method='box-cox')\npipe_trf = Pipeline([\n        ('transform', trf),\n        ('clf', clf)])\n\n\nnp.mean(cross_val_score(pipe_trf, Xp1, y, cv=cv, scoring = 'roc_auc'))\n\n0.9692078370914341\n\n\n\nX_trf = pd.DataFrame(trf.fit_transform(Xp1), index=X.index, columns=X.columns)\n\n\nlogreg_trf = sm.Logit(y,sm.add_constant(X_trf)).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.089226\n         Iterations 11\n\n\n\nlogreg_trf.summary().tables[0]\n\n\n\nLogit Regression Results\n\n  Dep. Variable:         Class        No. Observations:        844  \n\n\n  Model:                 Logit        Df Residuals:            786  \n\n\n  Method:                 MLE         Df Model:                 57  \n\n\n  Date:            Fri, 20 May 2022   Pseudo R-squ.:        0.8693  \n\n\n  Time:                10:33:16       Log-Likelihood:       -75.307 \n\n\n  converged:             True         LL-Null:              -576.17 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        9.702e-173\n\n\n\n\n\npd.read_html(logreg_trf.summary().tables[1].as_html(), header=0, index_col=0)[0]\n\n\n\n\n\n  \n    \n      \n      coef\n      std err\n      z\n      P>|z|\n      [0.025\n      0.975]\n    \n  \n  \n    \n      const\n      4.5543\n      0.743\n      6.130\n      0.000\n      3.098\n      6.011\n    \n    \n      Actinin\n      -0.8145\n      0.400\n      -2.038\n      0.042\n      -1.598\n      -0.031\n    \n    \n      ASCA\n      -0.2700\n      0.301\n      -0.896\n      0.370\n      -0.861\n      0.321\n    \n    \n      Beta2GP1\n      -0.4904\n      0.309\n      -1.589\n      0.112\n      -1.095\n      0.114\n    \n    \n      C1q\n      -0.4621\n      0.425\n      -1.088\n      0.277\n      -1.294\n      0.370\n    \n    \n      C3b\n      -0.9447\n      0.456\n      -2.072\n      0.038\n      -1.838\n      -0.051\n    \n    \n      Cardiolipin\n      -0.2555\n      0.286\n      -0.893\n      0.372\n      -0.816\n      0.305\n    \n    \n      CCP1arg\n      -1.0606\n      0.446\n      -2.376\n      0.018\n      -1.936\n      -0.186\n    \n    \n      CCP1cit\n      0.1408\n      0.515\n      0.273\n      0.785\n      -0.869\n      1.150\n    \n    \n      CENP\n      -0.1280\n      0.269\n      -0.476\n      0.634\n      -0.655\n      0.399\n    \n    \n      CMV\n      -0.2295\n      0.295\n      -0.779\n      0.436\n      -0.807\n      0.348\n    \n    \n      CollagenII\n      0.7417\n      0.326\n      2.275\n      0.023\n      0.103\n      1.381\n    \n    \n      CpGmot\n      0.0019\n      0.711\n      0.003\n      0.998\n      -1.392\n      1.395\n    \n    \n      CRP1\n      0.3503\n      0.441\n      0.795\n      0.427\n      -0.513\n      1.214\n    \n    \n      DFS70\n      -0.1203\n      0.341\n      -0.353\n      0.724\n      -0.788\n      0.548\n    \n    \n      dsDNA2\n      5.3738\n      1.051\n      5.115\n      0.000\n      3.315\n      7.433\n    \n    \n      Enolasearg\n      0.5916\n      0.370\n      1.598\n      0.110\n      -0.134\n      1.317\n    \n    \n      Enolasecit\n      -0.5567\n      0.569\n      -0.978\n      0.328\n      -1.672\n      0.558\n    \n    \n      EphB2\n      -0.0102\n      0.391\n      -0.026\n      0.979\n      -0.777\n      0.756\n    \n    \n      FcER\n      0.1500\n      0.294\n      0.510\n      0.610\n      -0.427\n      0.727\n    \n    \n      Fibrillarin\n      -0.2806\n      0.306\n      -0.917\n      0.359\n      -0.880\n      0.319\n    \n    \n      Ficolin\n      0.8266\n      0.368\n      2.245\n      0.025\n      0.105\n      1.548\n    \n    \n      GAPDH\n      -0.3229\n      0.321\n      -1.007\n      0.314\n      -0.951\n      0.306\n    \n    \n      GBM\n      -1.2108\n      0.378\n      -3.206\n      0.001\n      -1.951\n      -0.470\n    \n    \n      H2Bp\n      0.5583\n      0.548\n      1.018\n      0.309\n      -0.516\n      1.633\n    \n    \n      H2Bpac\n      0.0365\n      0.509\n      0.072\n      0.943\n      -0.962\n      1.035\n    \n    \n      H4p\n      -1.0386\n      0.481\n      -2.161\n      0.031\n      -1.981\n      -0.097\n    \n    \n      H4pac\n      0.1825\n      0.369\n      0.495\n      0.620\n      -0.540\n      0.905\n    \n    \n      Histones\n      -2.0035\n      0.514\n      -3.896\n      0.000\n      -3.011\n      -0.996\n    \n    \n      IFNLambda\n      0.5550\n      0.336\n      1.650\n      0.099\n      -0.104\n      1.214\n    \n    \n      IFNOmega\n      0.2533\n      0.269\n      0.941\n      0.347\n      -0.274\n      0.781\n    \n    \n      Jo1\n      -0.0686\n      0.352\n      -0.195\n      0.845\n      -0.758\n      0.621\n    \n    \n      Ku\n      -0.3340\n      0.353\n      -0.945\n      0.344\n      -1.027\n      0.359\n    \n    \n      LaSSB\n      2.3330\n      0.495\n      4.713\n      0.000\n      1.363\n      3.303\n    \n    \n      MBL2\n      0.0647\n      0.318\n      0.203\n      0.839\n      -0.559\n      0.689\n    \n    \n      Mi2\n      0.2443\n      0.320\n      0.762\n      0.446\n      -0.384\n      0.872\n    \n    \n      Nucleosome\n      0.0982\n      0.619\n      0.159\n      0.874\n      -1.116\n      1.312\n    \n    \n      PCNA\n      -0.1289\n      0.498\n      -0.259\n      0.796\n      -1.105\n      0.847\n    \n    \n      Pentraxin3\n      1.0234\n      0.355\n      2.884\n      0.004\n      0.328\n      1.719\n    \n    \n      PmScl100\n      0.1213\n      0.286\n      0.425\n      0.671\n      -0.439\n      0.681\n    \n    \n      RA33\n      1.1406\n      0.439\n      2.601\n      0.009\n      0.281\n      2.000\n    \n    \n      RipP0\n      0.0648\n      0.446\n      0.145\n      0.884\n      -0.809\n      0.938\n    \n    \n      RipP0peptide\n      -2.9943\n      0.597\n      -5.018\n      0.000\n      -4.164\n      -1.825\n    \n    \n      RipP1\n      0.4435\n      0.678\n      0.654\n      0.513\n      -0.886\n      1.773\n    \n    \n      RipP2\n      1.5795\n      0.566\n      2.791\n      0.005\n      0.470\n      2.689\n    \n    \n      RNAPolIII\n      -0.0588\n      0.302\n      -0.195\n      0.846\n      -0.651\n      0.534\n    \n    \n      RNP70\n      -0.4699\n      0.370\n      -1.270\n      0.204\n      -1.195\n      0.255\n    \n    \n      RNPA\n      0.4044\n      0.294\n      1.374\n      0.170\n      -0.173\n      0.981\n    \n    \n      RNPC\n      -0.0150\n      0.350\n      -0.043\n      0.966\n      -0.700\n      0.670\n    \n    \n      Ro52\n      0.9610\n      0.478\n      2.011\n      0.044\n      0.025\n      1.897\n    \n    \n      Ro60\n      2.2896\n      0.535\n      4.281\n      0.000\n      1.241\n      3.338\n    \n    \n      RPP25ThTo\n      -0.3136\n      0.247\n      -1.268\n      0.205\n      -0.798\n      0.171\n    \n    \n      Scl70\n      -0.9654\n      0.345\n      -2.802\n      0.005\n      -1.641\n      -0.290\n    \n    \n      SmBB\n      1.2639\n      0.421\n      3.001\n      0.003\n      0.438\n      2.089\n    \n    \n      SMP\n      -0.7764\n      0.336\n      -2.307\n      0.021\n      -1.436\n      -0.117\n    \n    \n      TIF1gamma\n      0.7880\n      0.352\n      2.237\n      0.025\n      0.098\n      1.479\n    \n    \n      TPO\n      0.3571\n      0.277\n      1.291\n      0.197\n      -0.185\n      0.899\n    \n    \n      tTG\n      -1.8544\n      0.399\n      -4.649\n      0.000\n      -2.636\n      -1.073\n    \n  \n\n\n\n\n\ncoefs_trf = coef_from_sm(logreg_trf)\n\n\ncoef_plot(coefs_trf, OR=False) # plotting ORs here is not feasible as dsDNA is so large\n\n\n\n\nNote that once normalized in this way, coefficient for CpGmot becomes very small!"
  },
  {
    "objectID": "notebooks/SLE Versus BBD.html#penalized-logistic-regression",
    "href": "notebooks/SLE Versus BBD.html#penalized-logistic-regression",
    "title": "Models for discriminating SLE patients from healthy controls (Blood Bank Donors)",
    "section": "Penalized logistic regression",
    "text": "Penalized logistic regression\n\ntrf = PowerTransformer(method='box-cox')\n\n\nRidge\nChoose range for regularization:\n\n# for ridge, pick alpha close to zero (if zero, max lambda is infinite. pick alpha=1e-3 to get same as glmnet) \n# and widen the range between lambda max and min (epsilon)\nlambda_min, lambda_max = regularization_range(Xp1,y,trf,alpha=1e-2, epsilon=1e-6)\n\nChoose a final value for the regularization parameter with the 1 SE rule (as just choosing the best lambda tends to under-regularize, https://stats.stackexchange.com/questions/138569)\n\nclf_ridge = LogisticRegression(penalty='l2', max_iter = 10000)\nK = 100\nCs_ridge = np.logspace(np.log10(1/lambda_min), np.log10(1/lambda_max), K) # C is inverse of lambda\npipe = Pipeline([\n        ('trf', trf),\n        ('clf', clf_ridge)\n])\nparams = [{\n    \"clf__C\": Cs_ridge\n}]\n\nsearch_ridge = GridSearchCV(pipe, params, cv = cv, scoring = 'roc_auc', refit=choose_C)\n\n\n%%time\nsearch_ridge.fit(Xp1,y)\n\nCPU times: user 8min 54s, sys: 13min 30s, total: 22min 24s\nWall time: 6min 10s\n\n\nGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=40),\n             estimator=Pipeline(steps=[('trf',\n                                        PowerTransformer(method='box-cox')),\n                                       ('clf',\n                                        LogisticRegression(max_iter=10000))]),\n             param_grid=[{'clf__C': array([3.42342671e+01, 2.97752197e+01, 2.58969676e+01, 2.25238618e+01,\n       1.95901063e+01, 1.70384754e+01, 1.48191970e+01, 1.28889818e+01,\n       1.12101...\n       4.85261892e-04, 4.22056046e-04, 3.67082825e-04, 3.19269921e-04,\n       2.77684695e-04, 2.41515987e-04, 2.10058289e-04, 1.82697987e-04,\n       1.58901392e-04, 1.38204327e-04, 1.20203076e-04, 1.04546505e-04,\n       9.09292187e-05, 7.90855973e-05, 6.87846194e-05, 5.98253541e-05,\n       5.20330420e-05, 4.52556864e-05, 3.93610881e-05, 3.42342671e-05])}],\n             refit=<function choose_C at 0x7fecee7479e0>, scoring='roc_auc')\n\n\nBest model:\n\nsearch_ridge.cv_results_['mean_test_score'].max()\n\n0.9784842809330654\n\n\nScore with lambda selected through 1 SE rule:\n\nsearch_ridge.cv_results_['mean_test_score'][search_ridge.best_index_]\n\n0.9766731818847515\n\n\nPlot coefficients for different levels of regularization:\n\ncoefs_ridge, nnz_coefs_ridge = regularization_path(Cs_ridge, clf_ridge, X_trf, y)\n\n\nax1, ax2, ax22 = plot_regularization_path(1/Cs_ridge, coefs_ridge, nnz_coefs_ridge, search_ridge.cv_results_)\n\n\n\n\nWith nested cross-validation:\n\n#np.mean(cross_val_score(search_ridge, Xp1, y, cv=cv, scoring = 'roc_auc'))\n\n\n\nLASSO\n\nlambda_min, lambda_max = regularization_range(Xp1,y,trf)\n\n\nclf_lasso = LogisticRegression(penalty='l1', max_iter = 10000, solver = 'liblinear')\nK = 100\nCs_lasso = np.logspace(np.log10(1/lambda_min),np.log10(1/lambda_max), K)\n#Cs_lasso = l1_min_c(trf.fit_transform(Xp1), y, loss='log') * np.logspace(0, 3, 100)\npipe = Pipeline([\n        ('trf', trf),\n        ('clf', clf_lasso)\n])\nparams = [{\n    \"clf__C\": Cs_lasso\n}]\n\nsearch_lasso = GridSearchCV(pipe, params, cv = cv, scoring = 'roc_auc', refit=choose_C)\n\n\n%%time\nsearch_lasso.fit(Xp1,y)\n\nCPU times: user 8min 31s, sys: 14min 55s, total: 23min 27s\nWall time: 4min 52s\n\n\nGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=40),\n             estimator=Pipeline(steps=[('trf',\n                                        PowerTransformer(method='box-cox')),\n                                       ('clf',\n                                        LogisticRegression(max_iter=10000,\n                                                           penalty='l1',\n                                                           solver='liblinear'))]),\n             param_grid=[{'clf__C': array([3.42342671, 3.19269921, 2.97752197, 2.77684695, 2.58969676,\n       2.41515987, 2.25238618, 2.10058289, 1.95901...\n       0.02589697, 0.0241516 , 0.02252386, 0.02100583, 0.01959011,\n       0.0182698 , 0.01703848, 0.01589014, 0.0148192 , 0.01382043,\n       0.01288898, 0.01202031, 0.01121018, 0.01045465, 0.00975004,\n       0.00909292, 0.00848009, 0.00790856, 0.00737555, 0.00687846,\n       0.00641488, 0.00598254, 0.00557933, 0.0052033 , 0.00485262,\n       0.00452557, 0.00422056, 0.00393611, 0.00367083, 0.00342343])}],\n             refit=<function choose_C at 0x7fecee7479e0>, scoring='roc_auc')\n\n\nBest model:\n\nsearch_lasso.cv_results_['mean_test_score'].max()\n\n0.9829858422316895\n\n\nScore with lambda selected through 1 SE rule:\n\nsearch_lasso.cv_results_['mean_test_score'][search_lasso.best_index_]\n\n0.9813334351642632\n\n\n\ncoefs_lasso, nnz_coefs_lasso = regularization_path(Cs_lasso, clf_lasso, X_trf, y)\n\n/home/lcreteig/miniconda3/envs/SLE/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n\n\n\nax1, ax2, ax22 = plot_regularization_path(1/Cs_lasso, coefs_lasso, nnz_coefs_lasso, search_lasso.cv_results_)\nax22.set_ylim([.9, 1])\n\n(0.9, 1)\n\n\n\n\n\nPlot the coefficients for models with different levels of regularization (= number of features retained). Start with the model with the best CV ROC AUC, and work our way down to include less and less features:\n\ncoef_plots_regularized(coefs_lasso, nnz_coefs_lasso, search_lasso.cv_results_[\"mean_test_score\"], X.columns)\n\n\n\n\nAlthough a lambda that leaves 23 features seems best, AUC only really starts to drop at around 15 features.\nNon-zero coefficients of the model selected with the 1SE rule:\n\ncoefs_final = (pd.Series(search_lasso.best_estimator_.named_steps.clf.coef_.squeeze(), index = X.columns)\n .where(lambda x : x!=0).\n dropna()\n .sort_values())\ncoefs_final\n\ntTG            -0.612562\nRipP0peptide   -0.468708\nHistones       -0.436773\nCCP1arg        -0.321419\nGBM            -0.256901\nPmScl100        0.016689\nRNPA            0.060150\nPentraxin3      0.063666\nRipP2           0.073611\nTIF1gamma       0.193812\nRo52            0.278128\nLaSSB           0.433654\nSmBB            0.482324\nRo60            1.093204\ndsDNA2          1.113356\ndtype: float64\n\n\nTable with all coefficients:\n\nmake_coef_tbl(logreg_trf, search_lasso.best_estimator_.named_steps.clf, X.columns)\n\n\n\n\n\n  \n    \n      \n      coef_regularized\n      coef\n      std err\n      95%CI\n    \n  \n  \n    \n      const\n      0.760\n      4.554\n      0.743\n      3.098, 6.011\n    \n    \n      Actinin\n      0.000\n      -0.814\n      0.400\n      -1.598, -0.031\n    \n    \n      ASCA\n      0.000\n      -0.270\n      0.301\n      -0.861, 0.321\n    \n    \n      Beta2GP1\n      0.000\n      -0.490\n      0.309\n      -1.095, 0.114\n    \n    \n      C1q\n      0.000\n      -0.462\n      0.425\n      -1.294, 0.37\n    \n    \n      C3b\n      0.000\n      -0.945\n      0.456\n      -1.838, -0.051\n    \n    \n      Cardiolipin\n      0.000\n      -0.256\n      0.286\n      -0.816, 0.305\n    \n    \n      CCP1arg\n      -0.321\n      -1.061\n      0.446\n      -1.936, -0.186\n    \n    \n      CCP1cit\n      0.000\n      0.141\n      0.515\n      -0.869, 1.15\n    \n    \n      CENP\n      0.000\n      -0.128\n      0.269\n      -0.655, 0.399\n    \n    \n      CMV\n      0.000\n      -0.230\n      0.295\n      -0.807, 0.348\n    \n    \n      CollagenII\n      0.000\n      0.742\n      0.326\n      0.103, 1.381\n    \n    \n      CpGmot\n      0.000\n      0.002\n      0.711\n      -1.392, 1.395\n    \n    \n      CRP1\n      0.000\n      0.350\n      0.441\n      -0.513, 1.214\n    \n    \n      DFS70\n      0.000\n      -0.120\n      0.341\n      -0.788, 0.548\n    \n    \n      dsDNA2\n      1.113\n      5.374\n      1.051\n      3.315, 7.433\n    \n    \n      Enolasearg\n      0.000\n      0.592\n      0.370\n      -0.134, 1.317\n    \n    \n      Enolasecit\n      0.000\n      -0.557\n      0.569\n      -1.672, 0.558\n    \n    \n      EphB2\n      0.000\n      -0.010\n      0.391\n      -0.777, 0.756\n    \n    \n      FcER\n      0.000\n      0.150\n      0.294\n      -0.427, 0.727\n    \n    \n      Fibrillarin\n      0.000\n      -0.281\n      0.306\n      -0.88, 0.319\n    \n    \n      Ficolin\n      0.000\n      0.827\n      0.368\n      0.105, 1.548\n    \n    \n      GAPDH\n      0.000\n      -0.323\n      0.321\n      -0.951, 0.306\n    \n    \n      GBM\n      -0.257\n      -1.211\n      0.378\n      -1.951, -0.47\n    \n    \n      H2Bp\n      0.000\n      0.558\n      0.548\n      -0.516, 1.633\n    \n    \n      H2Bpac\n      0.000\n      0.036\n      0.509\n      -0.962, 1.035\n    \n    \n      H4p\n      0.000\n      -1.039\n      0.481\n      -1.981, -0.097\n    \n    \n      H4pac\n      0.000\n      0.182\n      0.369\n      -0.54, 0.905\n    \n    \n      Histones\n      -0.437\n      -2.003\n      0.514\n      -3.011, -0.996\n    \n    \n      IFNLambda\n      0.000\n      0.555\n      0.336\n      -0.104, 1.214\n    \n    \n      IFNOmega\n      0.000\n      0.253\n      0.269\n      -0.274, 0.781\n    \n    \n      Jo1\n      0.000\n      -0.069\n      0.352\n      -0.758, 0.621\n    \n    \n      Ku\n      0.000\n      -0.334\n      0.353\n      -1.027, 0.359\n    \n    \n      LaSSB\n      0.434\n      2.333\n      0.495\n      1.363, 3.303\n    \n    \n      MBL2\n      0.000\n      0.065\n      0.318\n      -0.559, 0.689\n    \n    \n      Mi2\n      0.000\n      0.244\n      0.320\n      -0.384, 0.872\n    \n    \n      Nucleosome\n      0.000\n      0.098\n      0.619\n      -1.116, 1.312\n    \n    \n      PCNA\n      0.000\n      -0.129\n      0.498\n      -1.105, 0.847\n    \n    \n      Pentraxin3\n      0.064\n      1.023\n      0.355\n      0.328, 1.719\n    \n    \n      PmScl100\n      0.017\n      0.121\n      0.286\n      -0.439, 0.681\n    \n    \n      RA33\n      0.000\n      1.141\n      0.439\n      0.281, 2.0\n    \n    \n      RipP0\n      0.000\n      0.065\n      0.446\n      -0.809, 0.938\n    \n    \n      RipP0peptide\n      -0.469\n      -2.994\n      0.597\n      -4.164, -1.825\n    \n    \n      RipP1\n      0.000\n      0.444\n      0.678\n      -0.886, 1.773\n    \n    \n      RipP2\n      0.074\n      1.580\n      0.566\n      0.47, 2.689\n    \n    \n      RNAPolIII\n      0.000\n      -0.059\n      0.302\n      -0.651, 0.534\n    \n    \n      RNP70\n      0.000\n      -0.470\n      0.370\n      -1.195, 0.255\n    \n    \n      RNPA\n      0.060\n      0.404\n      0.294\n      -0.173, 0.981\n    \n    \n      RNPC\n      0.000\n      -0.015\n      0.350\n      -0.7, 0.67\n    \n    \n      Ro52\n      0.278\n      0.961\n      0.478\n      0.025, 1.897\n    \n    \n      Ro60\n      1.093\n      2.290\n      0.535\n      1.241, 3.338\n    \n    \n      RPP25ThTo\n      0.000\n      -0.314\n      0.247\n      -0.798, 0.171\n    \n    \n      Scl70\n      0.000\n      -0.965\n      0.345\n      -1.641, -0.29\n    \n    \n      SmBB\n      0.482\n      1.264\n      0.421\n      0.438, 2.089\n    \n    \n      SMP\n      0.000\n      -0.776\n      0.336\n      -1.436, -0.117\n    \n    \n      TIF1gamma\n      0.194\n      0.788\n      0.352\n      0.098, 1.479\n    \n    \n      TPO\n      0.000\n      0.357\n      0.277\n      -0.185, 0.899\n    \n    \n      tTG\n      -0.613\n      -1.854\n      0.399\n      -2.636, -1.073\n    \n  \n\n\n\n\n\nsns.reset_defaults()\npal = sns.diverging_palette(250, 15, s=75, l=40, center=\"dark\", as_cmap=True)\nwith sns.plotting_context(\"paper\"):\n    with sns.axes_style(\"whitegrid\"):\n        fig, ax = plt.subplots(figsize=(2.5, 2.5))\n        g = sns.scatterplot(y=coefs_final.index, x=coefs_final.values, \n                            hue=coefs_final.values, palette=pal, legend=False)\n        ax.set(title='SLE vs. BBD',\n               xticks=np.linspace(-1.0,1.0,5), xlim=[-1.2,1.2], xlabel=r'$\\beta$')\n        sns.despine(fig=fig,ax=ax, left=True, bottom=True)\n        #fig.savefig('coefs_SLE_BBD.png', bbox_inches='tight', dpi=300, transparent=True)\n        #fig.savefig('coefs_SLE_BBD.pdf', bbox_inches='tight', transparent=True)\n\n\n\n\nPlot cross-validated ROC:\n\nsns.reset_defaults()\nwith sns.plotting_context(\"paper\"):\n    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n    tprs, aucs = calc_roc_cv(search_lasso.best_estimator_,cv,Xp1,y)\n    fig, ax = plot_roc_cv(tprs, aucs, fig, ax, fig_title='SLE vs. BBD', line_color='#CC79A7', legend_label='LASSO')\n    tprs, aucs = calc_roc_cv(lr_dsDNA,cv,dsDNA,y)\n    fig, ax = plot_roc_cv(tprs, aucs, fig, ax, reuse=True, line_color='gray', legend_label='dsDNA only')\n    sns.despine(fig=fig,ax=ax, trim=True)\n    plt.legend(frameon=False)\n    plt.show()\n    #fig.savefig('roc_SLE_BBD.png', bbox_inches='tight', dpi=300, transparent=True)\n    #fig.savefig('roc_SLE_BBD.pdf', bbox_inches='tight', dpi=300, transparent=True)\n\n\n\n\n\nStability selection\nIf we pick a certain lambda value, the features that get included might vary with a different dataset. We can estimate that variability by taking bootstrap samples of the dataset. Then we can select the features that are included most often\n\n%%time\nselector = StabilitySelection(base_estimator=pipe, lambda_name='clf__C',\n                                  lambda_grid=Cs_lasso[np.argmax(search_lasso.cv_results_[\"mean_test_score\"]):], #range from highest scoring lambda to lambda_max\n                                  random_state=40) \nselector.fit(Xp1, y)\n\nCPU times: user 7min 37s, sys: 0 ns, total: 7min 37s\nWall time: 7min 37s\n\n\nStabilitySelection(base_estimator=Pipeline(steps=[('trf',\n                                                   PowerTransformer(method='box-cox')),\n                                                  ('clf',\n                                                   LogisticRegression(max_iter=10000,\n                                                                      penalty='l1',\n                                                                      solver='liblinear'))]),\n                   lambda_grid=array([0.13820433, 0.12888982, 0.12020308, 0.11210179, 0.10454651,\n       0.09750042, 0.09092922, 0.08480089, 0.0790856 , 0.07375549,\n       0.06878462, 0.06414877, 0.05982535, 0.05579333, 0.052033...\n       0.0241516 , 0.02252386, 0.02100583, 0.01959011, 0.0182698 ,\n       0.01703848, 0.01589014, 0.0148192 , 0.01382043, 0.01288898,\n       0.01202031, 0.01121018, 0.01045465, 0.00975004, 0.00909292,\n       0.00848009, 0.00790856, 0.00737555, 0.00687846, 0.00641488,\n       0.00598254, 0.00557933, 0.0052033 , 0.00485262, 0.00452557,\n       0.00422056, 0.00393611, 0.00367083, 0.00342343]),\n                   lambda_name='clf__C', random_state=40)\n\n\n\nthresh = .45\nplot_stability_path(selector, threshold_highlight=thresh)\n\n(<Figure size 640x480 with 1 Axes>,\n <matplotlib.axes._subplots.AxesSubplot at 0x7fecdb511e10>)\n\n\n\n\n\n\nselector.set_params(threshold=thresh)\nselected_variables = selector.get_support(indices=True)\nselected_scores = selector.stability_scores_.max(axis=1)\n\npd.DataFrame({'Variable': Xp1.columns[selected_variables], \n              'Stability score': selected_scores[selected_variables]}).sort_values(by='Stability score', ascending=False)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Stability score\n    \n  \n  \n    \n      0\n      CCP1arg\n      1.00\n    \n    \n      1\n      dsDNA2\n      1.00\n    \n    \n      3\n      Histones\n      1.00\n    \n    \n      4\n      LaSSB\n      1.00\n    \n    \n      7\n      RipP0peptide\n      1.00\n    \n    \n      11\n      Ro60\n      1.00\n    \n    \n      12\n      SmBB\n      1.00\n    \n    \n      14\n      tTG\n      1.00\n    \n    \n      2\n      GBM\n      0.99\n    \n    \n      10\n      Ro52\n      0.98\n    \n    \n      13\n      TIF1gamma\n      0.96\n    \n    \n      5\n      Pentraxin3\n      0.82\n    \n    \n      8\n      RipP2\n      0.69\n    \n    \n      9\n      RNPA\n      0.60\n    \n    \n      6\n      PmScl100\n      0.47\n    \n  \n\n\n\n\nThis overlaps completely with the LASSO model fit on the whole dataset\n\n\nTest on other groups\n\npre-SLE (vs. rest)LLD (vs. rest)IMID (vs. nonIMID)\n\n\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(search_lasso, X_test+1, y_test, 'preSLE', 'rest_large')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n  rest_large       0.96      0.23      0.36       462\n      preSLE       0.04      0.76      0.07        17\n\n    accuracy                           0.24       479\n   macro avg       0.50      0.49      0.22       479\nweighted avg       0.93      0.24      0.35       479\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. preSLE); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. preSLE); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\nSensitivity is a bit better than when only using dsDNA, but everything else is worse\n\n\n\nX_test, y_test = prep_data(X_test_df, 'LLD', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(search_lasso, X_test+1, y_test, 'LLD', 'rest_large')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n  rest_large       0.95      0.23      0.36       462\n         LLD       0.06      0.82      0.11        28\n\n    accuracy                           0.26       490\n   macro avg       0.51      0.52      0.24       490\nweighted avg       0.90      0.26      0.35       490\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. LLD); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. LLD); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\nSimilar to preSLE. Seems that when trained on blood bank controls, model classifies everyone as a patient\n\n\n\nX_test, y_test = prep_data(data_all, 'IMID', 'nonIMID', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(search_lasso, X_test+1, y_test, 'IMID', 'nonIMID')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n     nonIMID       0.49      0.21      0.29       218\n        IMID       0.63      0.86      0.73       346\n\n    accuracy                           0.61       564\n   macro avg       0.56      0.54      0.51       564\nweighted avg       0.58      0.61      0.56       564\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. IMID); specificity for the other group (nonIMID)\nN.B.: \"precision\" = PPV for the group in this row (e.g. IMID); NPV for the other group (nonIMID)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElastic net\n\nclf_enet = LogisticRegression(penalty='elasticnet', max_iter = 10000, solver = 'saga', n_jobs=-1)\nCs = 1/np.logspace(-3,3,20) # C is inverse of lambda\nl1l2 = np.linspace(0,1,10) # 0 = l2 only; 1 = l1 only\npipe = Pipeline([\n        ('trf', trf),\n        ('clf', clf_enet)\n])\nparams = [{\n    \"clf__C\": Cs,\n    \"clf__l1_ratio\": l1l2\n}]\n\nsearch_enet = GridSearchCV(pipe, params, cv = cv, scoring = 'roc_auc', n_jobs=-1)\n\n\n%%time\nwith parallel_backend('threading'):\n    search_enet.fit(Xp1,y)\n\nCPU times: user 1h 40min 45s, sys: 45min 40s, total: 2h 26min 25s\nWall time: 32min\n\n\n\nsearch_enet.best_params_\n\n{'clf__C': 0.16237767391887226, 'clf__l1_ratio': 0.7777777777777777}\n\n\nSo the best model tends to favor L1 more than L2, and has a similar lambda value compared to pure L1/L2\n\nsearch_enet.best_score_\n\n0.9833992316750093\n\n\nAUC is a bit higher, but similar to LASSO\n\n# Get coefficients for each C/l1_ratio\ncoefs_enet = np.empty([len(Cs), len(l1l2), len(X.columns)])\nfor i,c in enumerate(Cs):\n    for j,l in enumerate(l1l2):\n        clf_enet.set_params(C=c, l1_ratio = l, warm_start=True)\n        clf_enet.fit(X_trf,y)\n        coefs_enet[i,j,:] = clf_enet.coef_\n\n\n# Plot AUC and n_features for different lambdas\nscores = search_enet.cv_results_[\"mean_test_score\"].reshape(len(Cs),len(l1l2))\nlines = plt.plot(1/Cs, scores)\nplt.legend(lines, np.round(l1l2,2))\nplt.xscale('log')\nplt.xlim(1e0, 1e3)\nplt.ylim([0.97, .99])\nplt.xlabel('Lambda')\nplt.ylabel('ROC AUC')\nplt.title('Performance as a function of (Elastic net) regularization')\n\nText(0.5, 1.0, 'Performance as a function of (Elastic net) regularization')\n\n\n\n\n\nHere 0 = Ridge; 1 = LASSO. Max performance is very similar to pure LASSO, but with stronger regularization the hybrid models start to do a little better than pure Ridge or LASSO (but this is a small difference; note the scaling!)\n\nnp.sum(np.abs(coefs_enet[Cs == search_enet.best_params_[\"clf__C\"], l1l2 == search_enet.best_params_[\"clf__l1_ratio\"],:]) > 0)\n\n29\n\n\nThe best model retains 29 features\n\n\nHuber\n\nclf_huber = SGDClassifier(loss = 'modified_huber', penalty='elasticnet', max_iter = 10000)\nalpha = np.logspace(-3,3,20) # regularization strength\nl1l2 = np.linspace(0,1,10) # 0 = l2 only; 1 = l1 only\npipe = Pipeline([\n        ('trf', trf),\n        ('clf', clf_huber)\n])\nparams = [{\n    \"clf__alpha\": alpha,\n    \"clf__l1_ratio\": l1l2\n}]\n\nsearch_huber = GridSearchCV(pipe, params, cv = cv, scoring = 'roc_auc')\n\n\n%%time\nsearch_huber.fit(Xp1, y)\n\nCPU times: user 17min 55s, sys: 35min 56s, total: 53min 52s\nWall time: 10min 27s\n\n\nGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=40),\n             estimator=Pipeline(steps=[('trf',\n                                        PowerTransformer(method='box-cox')),\n                                       ('clf',\n                                        SGDClassifier(loss='modified_huber',\n                                                      max_iter=10000,\n                                                      penalty='elasticnet'))]),\n             param_grid=[{'clf__alpha': array([1.00000000e-03, 2.06913808e-03, 4.28133240e-03, 8.85866790e-03,\n       1.83298071e-02, 3.79269019....84759970e-02, 1.62377674e-01,\n       3.35981829e-01, 6.95192796e-01, 1.43844989e+00, 2.97635144e+00,\n       6.15848211e+00, 1.27427499e+01, 2.63665090e+01, 5.45559478e+01,\n       1.12883789e+02, 2.33572147e+02, 4.83293024e+02, 1.00000000e+03]),\n                          'clf__l1_ratio': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])}],\n             scoring='roc_auc')\n\n\n\nsearch_huber.best_score_\n\n0.9833815845219601\n\n\nThis is very comparable to LASSO / elastic net"
  },
  {
    "objectID": "notebooks/SLE Versus BBD.html#random-forest",
    "href": "notebooks/SLE Versus BBD.html#random-forest",
    "title": "Models for discriminating SLE patients from healthy controls (Blood Bank Donors)",
    "section": "Random Forest",
    "text": "Random Forest\n\nclf_rf = RandomForestClassifier(random_state=40)\nnp.mean(cross_val_score(clf_rf, X, y, cv=cv, scoring = 'roc_auc'))\n\n0.9912558838826646\n\n\nCompletely untuned random forest with no feature preprocessing does even better still, but the increase in AUC is not worth the decrease in interpretability… Note though that this performs even a bit better than the (extensively tuned) XGBoost!"
  },
  {
    "objectID": "notebooks/SLE Versus nonIMID.html",
    "href": "notebooks/SLE Versus nonIMID.html",
    "title": "Models for discriminating SLE patients from nonIMID controls",
    "section": "",
    "text": "Citation\n\n\n\nThis notebook contains analyses for the following project:\nBrunekreef TE, Reteig LC, Limper M, Haitjema S, Dias J, Mathsson-Alm L, van Laar JM, Otten HG. Microarray analysis of autoantibodies can identify future Systemic Lupus Erythematosus patients. Human Immunology. 2022 Apr 11. doi:10.1016/j.humimm.2022.03.010"
  },
  {
    "objectID": "notebooks/SLE Versus nonIMID.html#setup",
    "href": "notebooks/SLE Versus nonIMID.html#setup",
    "title": "Models for discriminating SLE patients from nonIMID controls",
    "section": "Setup",
    "text": "Setup\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport feather\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, plot_roc_curve\nfrom stability_selection import StabilitySelection, plot_stability_path\nfrom joblib import parallel_backend\n\nfrom sle.regression import coef_from_sm, coef_plot, make_coef_tbl\nfrom sle.modeling import generate_data, prep_data, eval_model, calc_roc_cv, plot_roc_cv\nfrom sle.penalization import regularization_range, choose_C, regularization_path, plot_regularization_path, coef_plots_regularized\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\n\n\nRunning the code without the original data\n\n\n\nIf you want to run the code but don’t have access to the data, run the following instead to generate some synthetic data:\n\n\ndata_all = generate_data('imid')\nX_test_df = generate_data('rest')\n\n\nCode for loading original data\ndata_dir = os.path.join('..', 'data', 'processed')\ndata_all = feather.read_dataframe(os.path.join(data_dir, 'imid.feather'))\nX_test_df = feather.read_dataframe(os.path.join(data_dir,'rest.feather'))\n\n\n\nX_nonIMID, y_nonIMID = prep_data(data_all, 'SLE', 'nonIMID', drop_cols = [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"])\ndsDNA_nonIMID = X_nonIMID.dsDNA2.values.reshape(-1,1) # only dsDNA from chip as a vector\ndsDNA_lab_nonIMID = X_nonIMID.dsDNA1.values.reshape(-1,1)\nX_nonIMID.drop(\"dsDNA1\", axis=1, inplace=True) # drop clinic dsDNA from multivariable data frame\n\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=40)"
  },
  {
    "objectID": "notebooks/SLE Versus nonIMID.html#logistic-regression-only-dsdna-from-microarray",
    "href": "notebooks/SLE Versus nonIMID.html#logistic-regression-only-dsdna-from-microarray",
    "title": "Models for discriminating SLE patients from nonIMID controls",
    "section": "Logistic regression: Only dsDNA from microarray",
    "text": "Logistic regression: Only dsDNA from microarray\n\nclf = LogisticRegression(penalty = 'none', max_iter = 10000) # increase iterations for solver to converge\n\n\nlr_dsDNA_nonIMID = clf.fit(dsDNA_nonIMID, y_nonIMID)\n\n\nnp.mean(cross_val_score(clf, dsDNA_nonIMID, y_nonIMID, cv=cv, scoring = 'roc_auc'))\n\n0.8006217102395325\n\n\nInterestingly, AUC for dsDNA only is around the same value as for the vs. blood bank controls, though this is a much harder classification problem. So dsDNA seems to be the most relevant in this set. Also, it’s barely lower than the XGBoost model…\n\nTest on pre-SLE (vs. rest)Test on LLD (vs. rest)Test on IMID (vs. blood bank)\n\n\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large')\neval_model(lr_dsDNA_nonIMID, X_test.dsDNA2.values.reshape(-1,1), y_test, 'preSLE', 'rest_large')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n  rest_large       0.99      0.34      0.51       462\n      preSLE       0.05      0.88      0.09        17\n\n    accuracy                           0.36       479\n   macro avg       0.52      0.61      0.30       479\nweighted avg       0.95      0.36      0.49       479\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. preSLE); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. preSLE); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\n\n\n\nX_test, y_test = prep_data(X_test_df, 'LLD', 'rest_large')\neval_model(lr_dsDNA_nonIMID, X_test.dsDNA2.values.reshape(-1,1), y_test, 'LLD', 'rest_large')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n  rest_large       0.98      0.34      0.50       462\n         LLD       0.08      0.89      0.14        28\n\n    accuracy                           0.37       490\n   macro avg       0.53      0.62      0.32       490\nweighted avg       0.93      0.37      0.48       490\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. LLD); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. LLD); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\nBoth for preSLE and LLD, sensitivity is great when only using dsDNA… But specificity is pretty bad\n\n\n\nX_test, y_test = prep_data(data_all, 'IMID', 'BBD')\neval_model(lr_dsDNA_nonIMID, X_test.dsDNA2.values.reshape(-1,1), y_test, 'IMID', 'BBD')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n         BBD       0.38      0.13      0.20       361\n        IMID       0.46      0.77      0.58       346\n\n    accuracy                           0.45       707\n   macro avg       0.42      0.45      0.39       707\nweighted avg       0.42      0.45      0.38       707\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. IMID); specificity for the other group (BBD)\nN.B.: \"precision\" = PPV for the group in this row (e.g. IMID); NPV for the other group (BBD)\n\n\n\n\n\n\n\n\n\nDoes not do so well even on healthy controls, so this model is just very sensitive to everything it seems"
  },
  {
    "objectID": "notebooks/SLE Versus nonIMID.html#logistic-regression-only-dsdna-from-clinic",
    "href": "notebooks/SLE Versus nonIMID.html#logistic-regression-only-dsdna-from-clinic",
    "title": "Models for discriminating SLE patients from nonIMID controls",
    "section": "Logistic regression: Only dsDNA from clinic",
    "text": "Logistic regression: Only dsDNA from clinic\n\nclf = LogisticRegression(penalty = 'none', max_iter = 10000) # increase iterations for solver to converge\n\n\nlr_dsDNA_lab_nonIMID = clf.fit(dsDNA_lab_nonIMID, y_nonIMID)\n\n\nnp.mean(cross_val_score(clf, dsDNA_lab_nonIMID, y_nonIMID, cv=cv, scoring = 'roc_auc'))\n\n0.8136558728921921\n\n\n\nTest on pre-SLE (vs. rest)Test on LLD (vs. rest)\n\n\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large')\neval_model(lr_dsDNA_lab_nonIMID, X_test.dsDNA1.values.reshape(-1,1), y_test, 'preSLE', 'rest_large')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n  rest_large       0.00      0.00      0.00       462\n      preSLE       0.04      1.00      0.07        17\n\n    accuracy                           0.04       479\n   macro avg       0.02      0.50      0.03       479\nweighted avg       0.00      0.04      0.00       479\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. preSLE); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. preSLE); NPV for the other group (rest_large)\n\n\n\n/home/lcreteig/miniconda3/envs/SLE/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n\n\n\n\n\n\nX_test, y_test = prep_data(X_test_df, 'LLD', 'rest_large')\neval_model(lr_dsDNA_lab_nonIMID, X_test.dsDNA1.values.reshape(-1,1), y_test, 'LLD', 'rest_large')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n  rest_large       0.00      0.00      0.00       462\n         LLD       0.06      1.00      0.11        28\n\n    accuracy                           0.06       490\n   macro avg       0.03      0.50      0.05       490\nweighted avg       0.00      0.06      0.01       490\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. LLD); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. LLD); NPV for the other group (rest_large)\n\n\n\n/home/lcreteig/miniconda3/envs/SLE/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))"
  },
  {
    "objectID": "notebooks/SLE Versus nonIMID.html#logistic-regression-without-dsdna",
    "href": "notebooks/SLE Versus nonIMID.html#logistic-regression-without-dsdna",
    "title": "Models for discriminating SLE patients from nonIMID controls",
    "section": "Logistic regression without dsDNA",
    "text": "Logistic regression without dsDNA\n\nno_dsDNA_nonIMID = X_nonIMID.drop(columns='dsDNA2')\n\n\nnp.mean(cross_val_score(clf, no_dsDNA_nonIMID, y_nonIMID, cv=cv, scoring = 'roc_auc'))\n\n0.7689850124052804\n\n\nA model with only dsDNA does better than one with all features but without dsDNA…"
  },
  {
    "objectID": "notebooks/SLE Versus nonIMID.html#logistic-regression-with-all-features",
    "href": "notebooks/SLE Versus nonIMID.html#logistic-regression-with-all-features",
    "title": "Models for discriminating SLE patients from nonIMID controls",
    "section": "Logistic regression with all features",
    "text": "Logistic regression with all features\nScale features to get rid of the skew\n\nXp1_nonIMID = X_nonIMID + 1 # Some < 0 values > -1. Because negative fluorescence isn't possible, and Box-Cox requires strictly positive values, add ofset\n\n\ntrf = PowerTransformer(method='box-cox')\npipe_trf = Pipeline([\n        ('transform', trf),\n        ('clf', clf)])\n\n\nnp.mean(cross_val_score(pipe_trf, Xp1_nonIMID, y_nonIMID, cv=cv, scoring = 'roc_auc'))\n\n0.8337294463757692\n\n\n\nX_trf_nonIMID = pd.DataFrame(trf.fit_transform(Xp1_nonIMID), index=X_nonIMID.index, columns=X_nonIMID.columns)\n\n\nlogreg_trf_nonIMID = sm.Logit(y_nonIMID,sm.add_constant(X_trf_nonIMID)).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.370490\n         Iterations 8\n\n\n\nlogreg_trf_nonIMID.summary().tables[0]\n\n\n\nLogit Regression Results\n\n  Dep. Variable:         Class        No. Observations:       701  \n\n\n  Model:                 Logit        Df Residuals:           643  \n\n\n  Method:                 MLE         Df Model:                57  \n\n\n  Date:            Fri, 20 May 2022   Pseudo R-squ.:       0.4023  \n\n\n  Time:                09:41:39       Log-Likelihood:      -259.71 \n\n\n  converged:             True         LL-Null:             -434.54 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        1.150e-43\n\n\n\n\n\npd.read_html(logreg_trf_nonIMID.summary().tables[1].as_html(), header=0, index_col=0)[0]\n\n\n\n\n\n  \n    \n      \n      coef\n      std err\n      z\n      P>|z|\n      [0.025\n      0.975]\n    \n  \n  \n    \n      const\n      1.7953\n      0.176\n      10.213\n      0.000\n      1.451\n      2.140\n    \n    \n      Actinin\n      -0.0771\n      0.149\n      -0.518\n      0.604\n      -0.369\n      0.214\n    \n    \n      ASCA\n      -0.2462\n      0.140\n      -1.763\n      0.078\n      -0.520\n      0.027\n    \n    \n      Beta2GP1\n      -0.2659\n      0.129\n      -2.057\n      0.040\n      -0.519\n      -0.013\n    \n    \n      C1q\n      -0.0068\n      0.152\n      -0.045\n      0.964\n      -0.305\n      0.291\n    \n    \n      C3b\n      0.1107\n      0.183\n      0.605\n      0.545\n      -0.248\n      0.469\n    \n    \n      Cardiolipin\n      -0.1269\n      0.134\n      -0.949\n      0.343\n      -0.389\n      0.135\n    \n    \n      CCP1arg\n      -0.0193\n      0.176\n      -0.110\n      0.912\n      -0.363\n      0.325\n    \n    \n      CCP1cit\n      0.4228\n      0.206\n      2.049\n      0.040\n      0.018\n      0.827\n    \n    \n      CENP\n      -0.1206\n      0.134\n      -0.903\n      0.367\n      -0.382\n      0.141\n    \n    \n      CMV\n      0.0332\n      0.141\n      0.236\n      0.813\n      -0.243\n      0.309\n    \n    \n      CollagenII\n      0.1021\n      0.165\n      0.620\n      0.535\n      -0.221\n      0.425\n    \n    \n      CpGmot\n      0.6923\n      0.252\n      2.751\n      0.006\n      0.199\n      1.185\n    \n    \n      CRP1\n      0.1281\n      0.175\n      0.732\n      0.464\n      -0.215\n      0.471\n    \n    \n      DFS70\n      -0.0569\n      0.137\n      -0.416\n      0.677\n      -0.325\n      0.211\n    \n    \n      dsDNA2\n      1.4226\n      0.226\n      6.284\n      0.000\n      0.979\n      1.866\n    \n    \n      Enolasearg\n      -0.2186\n      0.176\n      -1.238\n      0.216\n      -0.565\n      0.127\n    \n    \n      Enolasecit\n      -0.2687\n      0.200\n      -1.344\n      0.179\n      -0.661\n      0.123\n    \n    \n      EphB2\n      -0.5244\n      0.187\n      -2.806\n      0.005\n      -0.891\n      -0.158\n    \n    \n      FcER\n      -0.4159\n      0.143\n      -2.900\n      0.004\n      -0.697\n      -0.135\n    \n    \n      Fibrillarin\n      -0.2297\n      0.142\n      -1.619\n      0.105\n      -0.508\n      0.048\n    \n    \n      Ficolin\n      -0.0528\n      0.164\n      -0.323\n      0.747\n      -0.373\n      0.268\n    \n    \n      GAPDH\n      -0.2759\n      0.161\n      -1.710\n      0.087\n      -0.592\n      0.040\n    \n    \n      GBM\n      -0.2263\n      0.147\n      -1.540\n      0.123\n      -0.514\n      0.062\n    \n    \n      H2Bp\n      0.3658\n      0.206\n      1.774\n      0.076\n      -0.038\n      0.770\n    \n    \n      H2Bpac\n      -0.3061\n      0.211\n      -1.454\n      0.146\n      -0.719\n      0.107\n    \n    \n      H4p\n      -0.2371\n      0.167\n      -1.424\n      0.155\n      -0.564\n      0.089\n    \n    \n      H4pac\n      0.3143\n      0.169\n      1.854\n      0.064\n      -0.018\n      0.646\n    \n    \n      Histones\n      0.5177\n      0.196\n      2.635\n      0.008\n      0.133\n      0.903\n    \n    \n      IFNLambda\n      -0.1335\n      0.143\n      -0.935\n      0.350\n      -0.413\n      0.146\n    \n    \n      IFNOmega\n      -0.1391\n      0.137\n      -1.018\n      0.308\n      -0.407\n      0.129\n    \n    \n      Jo1\n      -0.1020\n      0.145\n      -0.705\n      0.481\n      -0.385\n      0.181\n    \n    \n      Ku\n      0.0382\n      0.150\n      0.255\n      0.799\n      -0.255\n      0.331\n    \n    \n      LaSSB\n      0.1743\n      0.173\n      1.009\n      0.313\n      -0.164\n      0.513\n    \n    \n      MBL2\n      0.1371\n      0.144\n      0.954\n      0.340\n      -0.145\n      0.419\n    \n    \n      Mi2\n      -0.3063\n      0.149\n      -2.050\n      0.040\n      -0.599\n      -0.014\n    \n    \n      Nucleosome\n      0.0451\n      0.221\n      0.204\n      0.838\n      -0.387\n      0.477\n    \n    \n      PCNA\n      0.0621\n      0.171\n      0.363\n      0.716\n      -0.273\n      0.397\n    \n    \n      Pentraxin3\n      0.3440\n      0.173\n      1.985\n      0.047\n      0.004\n      0.684\n    \n    \n      PmScl100\n      0.0535\n      0.132\n      0.405\n      0.686\n      -0.205\n      0.312\n    \n    \n      RA33\n      -0.1740\n      0.156\n      -1.115\n      0.265\n      -0.480\n      0.132\n    \n    \n      RipP0\n      0.5214\n      0.225\n      2.321\n      0.020\n      0.081\n      0.962\n    \n    \n      RipP0peptide\n      0.0165\n      0.172\n      0.096\n      0.923\n      -0.320\n      0.353\n    \n    \n      RipP1\n      0.1549\n      0.193\n      0.802\n      0.423\n      -0.224\n      0.533\n    \n    \n      RipP2\n      -0.0690\n      0.219\n      -0.315\n      0.753\n      -0.498\n      0.360\n    \n    \n      RNAPolIII\n      -0.0646\n      0.141\n      -0.459\n      0.646\n      -0.341\n      0.211\n    \n    \n      RNP70\n      -0.5786\n      0.170\n      -3.394\n      0.001\n      -0.913\n      -0.244\n    \n    \n      RNPA\n      0.1027\n      0.153\n      0.671\n      0.502\n      -0.198\n      0.403\n    \n    \n      RNPC\n      0.0975\n      0.149\n      0.654\n      0.513\n      -0.195\n      0.390\n    \n    \n      Ro52\n      -0.4573\n      0.218\n      -2.099\n      0.036\n      -0.884\n      -0.030\n    \n    \n      Ro60\n      0.4102\n      0.233\n      1.760\n      0.078\n      -0.047\n      0.867\n    \n    \n      RPP25ThTo\n      -0.2874\n      0.148\n      -1.941\n      0.052\n      -0.578\n      0.003\n    \n    \n      Scl70\n      -0.0538\n      0.145\n      -0.371\n      0.711\n      -0.338\n      0.231\n    \n    \n      SmBB\n      0.9490\n      0.203\n      4.682\n      0.000\n      0.552\n      1.346\n    \n    \n      SMP\n      -0.4842\n      0.189\n      -2.564\n      0.010\n      -0.854\n      -0.114\n    \n    \n      TIF1gamma\n      0.0212\n      0.145\n      0.147\n      0.883\n      -0.262\n      0.305\n    \n    \n      TPO\n      -0.2068\n      0.129\n      -1.607\n      0.108\n      -0.459\n      0.045\n    \n    \n      tTG\n      0.3488\n      0.131\n      2.669\n      0.008\n      0.093\n      0.605\n    \n  \n\n\n\n\n\ncoefs_trf_nonIMID = coef_from_sm(logreg_trf_nonIMID)\n\n\ncoef_plot(coefs_trf_nonIMID, OR=False)\n\n\n\n\nIn contrast to blood bank controls, there’s no quasi-complete separation in this model. CIs are wider and less predictors are significant. dsDNA and anti-Smith lead, which is to be expected. CpGmot also significant here. Ro60 quite a bit lower in the rank, and anti-correlated to Ro52…"
  },
  {
    "objectID": "notebooks/SLE Versus nonIMID.html#penalized-logistic-regression",
    "href": "notebooks/SLE Versus nonIMID.html#penalized-logistic-regression",
    "title": "Models for discriminating SLE patients from nonIMID controls",
    "section": "Penalized logistic regression",
    "text": "Penalized logistic regression\n\nRidge\n\nclf_ridge = LogisticRegression(penalty='l2', max_iter = 10000)\n\n\nK = 100\nlambda_min, lambda_max = regularization_range(Xp1_nonIMID,y_nonIMID,trf,alpha=1e-2, epsilon=1e-6)\nCs_ridge_nonIMID = np.logspace(np.log10(1/lambda_min), np.log10(1/lambda_max), K) # C is inverse of lambda\npipe = Pipeline([\n        ('trf', trf),\n        ('clf', clf_ridge)\n])\nparams = [{\n    \"clf__C\": Cs_ridge_nonIMID\n}]\n\nridge_nonIMID = GridSearchCV(pipe, params, cv = cv, scoring = 'roc_auc', refit=choose_C)\n\n\n%%time\nridge_nonIMID.fit(Xp1_nonIMID,y_nonIMID)\n\nCPU times: user 8min 37s, sys: 11min 49s, total: 20min 27s\nWall time: 6min 28s\n\n\nGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=40),\n             estimator=Pipeline(steps=[('trf',\n                                        PowerTransformer(method='box-cox')),\n                                       ('clf',\n                                        LogisticRegression(max_iter=10000))]),\n             param_grid=[{'clf__C': array([6.52012573e+01, 5.67087285e+01, 4.93223600e+01, 4.28980735e+01,\n       3.73105566e+01, 3.24508194e+01, 2.82240678e+01, 2.45478548e+01,\n       2.13504...\n       9.24210976e-04, 8.03831574e-04, 6.99131710e-04, 6.08069108e-04,\n       5.28867500e-04, 4.59981980e-04, 4.00068869e-04, 3.47959500e-04,\n       3.02637428e-04, 2.63218601e-04, 2.28934116e-04, 1.99115219e-04,\n       1.73180263e-04, 1.50623361e-04, 1.31004518e-04, 1.13941049e-04,\n       9.91001136e-05, 8.61922249e-05, 7.49656017e-05, 6.52012573e-05])}],\n             refit=<function choose_C at 0x7f7c2b3249e0>, scoring='roc_auc')\n\n\nBest model:\n\nridge_nonIMID.cv_results_['mean_test_score'].max()\n\n0.8381251384923316\n\n\nScore with lambda selected through 1 SE rule:\n\nridge_nonIMID.cv_results_['mean_test_score'][ridge_nonIMID.best_index_]\n\n0.8333156017741549\n\n\n\ncoefs_ridge_nonIMID, nnz_coefs_ridge_nonIMID = regularization_path(Cs_ridge_nonIMID, clf_ridge, X_trf_nonIMID, y_nonIMID)\n\n\nplot_regularization_path(1/Cs_ridge_nonIMID, coefs_ridge_nonIMID, nnz_coefs_ridge_nonIMID, ridge_nonIMID.cv_results_)\n\n(<matplotlib.axes._subplots.AxesSubplot at 0x7f7c207dc690>,\n <matplotlib.axes._subplots.AxesSubplot at 0x7f7c2292dc10>,\n <matplotlib.axes._subplots.AxesSubplot at 0x7f7c20789610>)\n\n\n\n\n\n\n\nLASSO\n\nclf_lasso = LogisticRegression(penalty='l1', max_iter = 10000, solver = 'liblinear')\n\n\nK = 100\nlambda_min, lambda_max = regularization_range(Xp1_nonIMID,y_nonIMID,trf)\nCs_lasso_nonIMID = np.logspace(np.log10(1/lambda_min),np.log10(1/lambda_max), K)\npipe = Pipeline([\n        ('trf', trf),\n        ('clf', clf_lasso)\n])\nparams = [{\n    \"clf__C\": Cs_lasso_nonIMID\n}]\n\nlasso_nonIMID = GridSearchCV(pipe, params, cv = cv, scoring = 'roc_auc', refit=choose_C)\n\n\n%%time\nlasso_nonIMID.fit(Xp1_nonIMID,y_nonIMID)\n\nCPU times: user 4min 47s, sys: 0 ns, total: 4min 47s\nWall time: 4min 47s\n\n\nGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=40),\n             estimator=Pipeline(steps=[('trf',\n                                        PowerTransformer(method='box-cox')),\n                                       ('clf',\n                                        LogisticRegression(max_iter=10000,\n                                                           penalty='l1',\n                                                           solver='liblinear'))]),\n             param_grid=[{'clf__C': array([6.52012573, 6.08069108, 5.67087285, 5.288675  , 4.932236  ,\n       4.5998198 , 4.28980735, 4.00068869, 3.73105566, 3...\n       0.04932236, 0.0459982 , 0.04289807, 0.04000689, 0.03731056,\n       0.03479595, 0.03245082, 0.03026374, 0.02822407, 0.02632186,\n       0.02454785, 0.02289341, 0.02135047, 0.01991152, 0.01856955,\n       0.01731803, 0.01615085, 0.01506234, 0.01404719, 0.01310045,\n       0.01221753, 0.0113941 , 0.01062618, 0.00991001, 0.00924211,\n       0.00861922, 0.00803832, 0.00749656, 0.00699132, 0.00652013])}],\n             refit=<function choose_C at 0x7f7c2b3249e0>, scoring='roc_auc')\n\n\nBest model:\n\nlasso_nonIMID.cv_results_['mean_test_score'].max()\n\n0.8373836641710802\n\n\nScore with lambda selected through 1 SE rule:\n\nlasso_nonIMID.cv_results_['mean_test_score'][lasso_nonIMID.best_index_]\n\n0.833423534978168\n\n\nHere LASSO is no longer better than Ridge\n\ncoefs_lasso_nonIMID, nnz_coefs_lasso_nonIMID = regularization_path(Cs_lasso_nonIMID, clf_lasso, X_trf_nonIMID, y_nonIMID)\n\n\nax1, ax2, ax22 = plot_regularization_path(1/Cs_lasso_nonIMID, coefs_lasso_nonIMID, nnz_coefs_lasso_nonIMID, lasso_nonIMID.cv_results_)\n#ax22.set_ylim([0.8, 1])\n\n\n\n\nSo best performance retains almost all features, but performance only drops less than .05 AUC with dramatically less features\n\ncoef_plots_regularized(coefs_lasso_nonIMID, nnz_coefs_lasso_nonIMID, lasso_nonIMID.cv_results_[\"mean_test_score\"], varnames=X_nonIMID.columns, num_subplot_cols=6)\n\n\n\n\nSo again, dsDNA is really the dominant feature here. But, to maximize performance, it might be best to regularize until about half of the original features are included, as performance starts to decline monotonically from that point forwards\n\nwith sns.plotting_context(\"paper\"):\n    plt.rcParams[\"font.family\"] = \"Arial\"\n    plt.rcParams['pdf.fonttype'] = 42 # this causes the pdf to be editable, but it also embeds the font, significantly increasing the size\n    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n    tprs, aucs = calc_roc_cv(lasso_nonIMID.best_estimator_,cv,Xp1_nonIMID,y_nonIMID)\n    fig, ax = plot_roc_cv(tprs, aucs, fig, ax, fig_title='SLE vs. non-IMID', line_color='#E69F00', legend_label='LASSO')\n    tprs, aucs = calc_roc_cv(lr_dsDNA_nonIMID,cv,dsDNA_nonIMID,y_nonIMID)\n    fig, ax = plot_roc_cv(tprs, aucs, fig, ax, reuse=True, line_color='gray', legend_label='dsDNA (from chip) only')\n    tprs, aucs = calc_roc_cv(lr_dsDNA_lab_nonIMID,cv,dsDNA_lab_nonIMID,y_nonIMID)\n    fig, ax = plot_roc_cv(tprs, aucs, fig, ax, reuse=True, line_color='black', legend_label='dsDNA (from clinic) only')\n    sns.despine(fig=fig,ax=ax, trim=True)\n    plt.legend(frameon=False)\n    plt.show()\n    fig.savefig('roc_SLE_nonIMID.png', bbox_inches='tight', dpi=300, transparent=True)\n    fig.savefig('roc_SLE_nonIMID.pdf', bbox_inches='tight', transparent=True)\n\n\n\n\nNon-zero coefficients of the model selected with the 1SE rule:\n\ncoefs_final_nonimid = (pd.Series(lasso_nonIMID.best_estimator_.named_steps.clf.coef_.squeeze(), \n                                 index = X_nonIMID.columns)[lambda x: x!=0].sort_values())\ncoefs_final_nonimid\n\nGBM           -0.164424\nEphB2         -0.141745\nBeta2GP1      -0.135576\nFibrillarin   -0.127466\nFcER          -0.117419\nRNP70         -0.114466\nEnolasearg    -0.112280\nASCA          -0.084297\nTPO           -0.079599\nRPP25ThTo     -0.076756\nJo1           -0.063222\nCENP          -0.055635\nIFNOmega      -0.052297\nIFNLambda     -0.043295\nEnolasecit    -0.042869\nMi2           -0.033627\nGAPDH         -0.023148\nActinin       -0.013356\nSMP           -0.007426\nC3b            0.009626\nLaSSB          0.025109\nRipP1          0.037874\nRo60           0.071421\ntTG            0.089406\nRipP0          0.097295\nCpGmot         0.215477\nHistones       0.336896\nSmBB           0.479428\ndsDNA2         0.958141\ndtype: float64\n\n\n\nlen(coefs_final_nonimid)\n\n29\n\n\n\nmake_coef_tbl(logreg_trf_nonIMID, lasso_nonIMID.best_estimator_.named_steps.clf, X_nonIMID.columns)\n\n\n\n\n\n  \n    \n      \n      coef_regularized\n      coef\n      std err\n      95%CI\n    \n  \n  \n    \n      const\n      1.101\n      1.795\n      0.176\n      1.451, 2.14\n    \n    \n      Actinin\n      -0.013\n      -0.077\n      0.149\n      -0.369, 0.214\n    \n    \n      ASCA\n      -0.084\n      -0.246\n      0.140\n      -0.52, 0.027\n    \n    \n      Beta2GP1\n      -0.136\n      -0.266\n      0.129\n      -0.519, -0.013\n    \n    \n      C1q\n      0.000\n      -0.007\n      0.152\n      -0.305, 0.291\n    \n    \n      C3b\n      0.010\n      0.111\n      0.183\n      -0.248, 0.469\n    \n    \n      Cardiolipin\n      0.000\n      -0.127\n      0.134\n      -0.389, 0.135\n    \n    \n      CCP1arg\n      0.000\n      -0.019\n      0.176\n      -0.363, 0.325\n    \n    \n      CCP1cit\n      0.000\n      0.423\n      0.206\n      0.018, 0.827\n    \n    \n      CENP\n      -0.056\n      -0.121\n      0.134\n      -0.382, 0.141\n    \n    \n      CMV\n      0.000\n      0.033\n      0.141\n      -0.243, 0.309\n    \n    \n      CollagenII\n      0.000\n      0.102\n      0.165\n      -0.221, 0.425\n    \n    \n      CpGmot\n      0.215\n      0.692\n      0.252\n      0.199, 1.185\n    \n    \n      CRP1\n      0.000\n      0.128\n      0.175\n      -0.215, 0.471\n    \n    \n      DFS70\n      0.000\n      -0.057\n      0.137\n      -0.325, 0.211\n    \n    \n      dsDNA2\n      0.958\n      1.423\n      0.226\n      0.979, 1.866\n    \n    \n      Enolasearg\n      -0.112\n      -0.219\n      0.176\n      -0.565, 0.127\n    \n    \n      Enolasecit\n      -0.043\n      -0.269\n      0.200\n      -0.661, 0.123\n    \n    \n      EphB2\n      -0.142\n      -0.524\n      0.187\n      -0.891, -0.158\n    \n    \n      FcER\n      -0.117\n      -0.416\n      0.143\n      -0.697, -0.135\n    \n    \n      Fibrillarin\n      -0.127\n      -0.230\n      0.142\n      -0.508, 0.048\n    \n    \n      Ficolin\n      0.000\n      -0.053\n      0.164\n      -0.373, 0.268\n    \n    \n      GAPDH\n      -0.023\n      -0.276\n      0.161\n      -0.592, 0.04\n    \n    \n      GBM\n      -0.164\n      -0.226\n      0.147\n      -0.514, 0.062\n    \n    \n      H2Bp\n      0.000\n      0.366\n      0.206\n      -0.038, 0.77\n    \n    \n      H2Bpac\n      0.000\n      -0.306\n      0.211\n      -0.719, 0.107\n    \n    \n      H4p\n      0.000\n      -0.237\n      0.167\n      -0.564, 0.089\n    \n    \n      H4pac\n      0.000\n      0.314\n      0.169\n      -0.018, 0.646\n    \n    \n      Histones\n      0.337\n      0.518\n      0.196\n      0.133, 0.903\n    \n    \n      IFNLambda\n      -0.043\n      -0.134\n      0.143\n      -0.413, 0.146\n    \n    \n      IFNOmega\n      -0.052\n      -0.139\n      0.137\n      -0.407, 0.129\n    \n    \n      Jo1\n      -0.063\n      -0.102\n      0.145\n      -0.385, 0.181\n    \n    \n      Ku\n      0.000\n      0.038\n      0.150\n      -0.255, 0.331\n    \n    \n      LaSSB\n      0.025\n      0.174\n      0.173\n      -0.164, 0.513\n    \n    \n      MBL2\n      0.000\n      0.137\n      0.144\n      -0.145, 0.419\n    \n    \n      Mi2\n      -0.034\n      -0.306\n      0.149\n      -0.599, -0.014\n    \n    \n      Nucleosome\n      0.000\n      0.045\n      0.221\n      -0.387, 0.477\n    \n    \n      PCNA\n      0.000\n      0.062\n      0.171\n      -0.273, 0.397\n    \n    \n      Pentraxin3\n      0.000\n      0.344\n      0.173\n      0.004, 0.684\n    \n    \n      PmScl100\n      0.000\n      0.054\n      0.132\n      -0.205, 0.312\n    \n    \n      RA33\n      0.000\n      -0.174\n      0.156\n      -0.48, 0.132\n    \n    \n      RipP0\n      0.097\n      0.521\n      0.225\n      0.081, 0.962\n    \n    \n      RipP0peptide\n      0.000\n      0.016\n      0.172\n      -0.32, 0.353\n    \n    \n      RipP1\n      0.038\n      0.155\n      0.193\n      -0.224, 0.533\n    \n    \n      RipP2\n      0.000\n      -0.069\n      0.219\n      -0.498, 0.36\n    \n    \n      RNAPolIII\n      0.000\n      -0.065\n      0.141\n      -0.341, 0.211\n    \n    \n      RNP70\n      -0.114\n      -0.579\n      0.170\n      -0.913, -0.244\n    \n    \n      RNPA\n      0.000\n      0.103\n      0.153\n      -0.198, 0.403\n    \n    \n      RNPC\n      0.000\n      0.098\n      0.149\n      -0.195, 0.39\n    \n    \n      Ro52\n      0.000\n      -0.457\n      0.218\n      -0.884, -0.03\n    \n    \n      Ro60\n      0.071\n      0.410\n      0.233\n      -0.047, 0.867\n    \n    \n      RPP25ThTo\n      -0.077\n      -0.287\n      0.148\n      -0.578, 0.003\n    \n    \n      Scl70\n      0.000\n      -0.054\n      0.145\n      -0.338, 0.231\n    \n    \n      SmBB\n      0.479\n      0.949\n      0.203\n      0.552, 1.346\n    \n    \n      SMP\n      -0.007\n      -0.484\n      0.189\n      -0.854, -0.114\n    \n    \n      TIF1gamma\n      0.000\n      0.021\n      0.145\n      -0.262, 0.305\n    \n    \n      TPO\n      -0.080\n      -0.207\n      0.129\n      -0.459, 0.045\n    \n    \n      tTG\n      0.089\n      0.349\n      0.131\n      0.093, 0.605\n    \n  \n\n\n\n\n\npal = sns.diverging_palette(250, 15, s=75, l=40, center=\"dark\", as_cmap=True)\nwith sns.plotting_context(\"paper\"):\n    with sns.axes_style(\"whitegrid\"):\n        fig, ax = plt.subplots(figsize=(2.5, 5))\n        g = sns.scatterplot(y=coefs_final_nonimid.index, x=coefs_final_nonimid.values, \n                            hue=coefs_final_nonimid.values, palette=pal, legend=False)\n        ax.set(title='SLE vs.non-IMID',\n               xticks=np.linspace(-1.0,1.0,5), xlim=[-1.2,1.2], xlabel=r'$\\beta$')\n        sns.despine(fig=fig,ax=ax, left=True, bottom=True)\n        #fig.savefig('coefs_SLE_nonIMID.png', bbox_inches='tight', dpi=300, transparent=True)\n        #fig.savefig('coefs_SLE_nonIMID.pdf', bbox_inches='tight', transparent=True)\n\n\n\n\n\nStability\nIn contrast to the blood bank controls, refitting with the chosen lambda to the whole dataset here seems to include much more features than during CV: 29 vs < 20. Might suggest that the included features are less stable. So let’s examine stability selection again:\n\n%%time\nselector_nonIMID = StabilitySelection(base_estimator=pipe, lambda_name='clf__C',\n                                  lambda_grid=Cs_lasso_nonIMID[np.argmax(lasso_nonIMID.cv_results_[\"mean_test_score\"]):],\n                                  random_state=40) # range from lambda with highest score to lambda_max\nselector_nonIMID.fit(Xp1_nonIMID, y_nonIMID)\n\nCPU times: user 9min 4s, sys: 0 ns, total: 9min 4s\nWall time: 9min 7s\n\n\nStabilitySelection(base_estimator=Pipeline(steps=[('trf',\n                                                   PowerTransformer(method='box-cox')),\n                                                  ('clf',\n                                                   LogisticRegression(max_iter=10000,\n                                                                      penalty='l1',\n                                                                      solver='liblinear'))]),\n                   lambda_grid=array([0.45998198, 0.42898073, 0.40006887, 0.37310557, 0.3479595 ,\n       0.32450819, 0.30263743, 0.28224068, 0.2632186 , 0.24547855,\n       0.22893412, 0.21350472, 0.19911522, 0.18569552, 0.1731802...\n       0.05670873, 0.05288675, 0.04932236, 0.0459982 , 0.04289807,\n       0.04000689, 0.03731056, 0.03479595, 0.03245082, 0.03026374,\n       0.02822407, 0.02632186, 0.02454785, 0.02289341, 0.02135047,\n       0.01991152, 0.01856955, 0.01731803, 0.01615085, 0.01506234,\n       0.01404719, 0.01310045, 0.01221753, 0.0113941 , 0.01062618,\n       0.00991001, 0.00924211, 0.00861922, 0.00803832, 0.00749656,\n       0.00699132, 0.00652013]),\n                   lambda_name='clf__C', random_state=40)\n\n\n\nthresh = .70\nplot_stability_path(selector_nonIMID, threshold_highlight=thresh)\n\n(<Figure size 432x288 with 1 Axes>,\n <matplotlib.axes._subplots.AxesSubplot at 0x7f7c392aae90>)\n\n\n\n\n\n\nselector_nonIMID.set_params(threshold=thresh)\nselected_variables = selector_nonIMID.get_support(indices=True)\nselected_scores = selector_nonIMID.stability_scores_.max(axis=1)\n\npd.DataFrame({'Variable': Xp1_nonIMID.columns[selected_variables], \n              'Stability score': selected_scores[selected_variables]}).sort_values(by='Stability score', ascending=False)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Stability score\n    \n  \n  \n    \n      27\n      SmBB\n      1.00\n    \n    \n      6\n      dsDNA2\n      1.00\n    \n    \n      16\n      Histones\n      1.00\n    \n    \n      24\n      RNP70\n      1.00\n    \n    \n      5\n      CpGmot\n      0.99\n    \n    \n      2\n      Beta2GP1\n      0.97\n    \n    \n      13\n      GBM\n      0.96\n    \n    \n      9\n      EphB2\n      0.95\n    \n    \n      10\n      FcER\n      0.94\n    \n    \n      11\n      Fibrillarin\n      0.94\n    \n    \n      1\n      ASCA\n      0.92\n    \n    \n      23\n      RipP0\n      0.92\n    \n    \n      26\n      RPP25ThTo\n      0.91\n    \n    \n      21\n      Mi2\n      0.90\n    \n    \n      29\n      TPO\n      0.89\n    \n    \n      7\n      Enolasearg\n      0.89\n    \n    \n      28\n      SMP\n      0.88\n    \n    \n      18\n      IFNOmega\n      0.84\n    \n    \n      30\n      tTG\n      0.84\n    \n    \n      17\n      IFNLambda\n      0.81\n    \n    \n      4\n      CENP\n      0.80\n    \n    \n      22\n      Pentraxin3\n      0.78\n    \n    \n      12\n      GAPDH\n      0.78\n    \n    \n      14\n      H2Bp\n      0.77\n    \n    \n      19\n      Jo1\n      0.75\n    \n    \n      8\n      Enolasecit\n      0.74\n    \n    \n      3\n      C3b\n      0.74\n    \n    \n      20\n      Ku\n      0.73\n    \n    \n      25\n      Ro60\n      0.72\n    \n    \n      0\n      Actinin\n      0.71\n    \n    \n      15\n      H4p\n      0.71\n    \n  \n\n\n\n\nMany of these seem to be selected as well by the 1SE LASSO model, but not all. For instance, the two variables with the lowest (positive) coefficients (RipP1, LaSSB) are missing here. At least in the case of RipP1, that might be because it’s highly correlated with other variables. When LASSO selects one of these, RipP1 will not be, driving down its inclusion probability.\nAll variables with a stability score of > .80 are included in the 1SE LASSO model, but below there are some differences\n\n\nTest on pre-SLE (vs. rest) group\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lasso_nonIMID, X_test+1, y_test, 'preSLE', 'rest_large')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n  rest_large       0.99      0.51      0.67       462\n      preSLE       0.06      0.88      0.12        17\n\n    accuracy                           0.52       479\n   macro avg       0.53      0.70      0.39       479\nweighted avg       0.96      0.52      0.65       479\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. preSLE); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. preSLE); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\nSensitivity is the same as when only using dsDNA, but specificity did improve quite a bit\nFigure 3:\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\n# ROC curve\nthreshold1=0.5\nthreshold2=0.84\nfpr, tpr, thresholds = roc_curve(y_test, lasso_nonIMID.predict_proba(X_test+1)[:,1])\nthr_idx1 = (np.abs(thresholds - threshold1)).argmin() # find index of value closest to chosen threshold\nthr_idx2 = (np.abs(thresholds - threshold2)).argmin() # find index of value closest to chosen threshold\nsns.reset_defaults()\nwith sns.plotting_context(\"paper\"):\n    plt.rcParams[\"font.family\"] = \"Arial\"\n    plt.rcParams['pdf.fonttype'] = 42 # this causes the pdf to be editable, but it also embeds the font, significantly increasing the size\n    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n    ax.plot([0, 1], [0, 1], linestyle='--', lw=1.5, color='k', alpha=.25)\n    ax.set(title=\"Pre-SLE vs. Rest\",\n           xlim=[-0.05, 1.05], xlabel='False positive rate', \n           ylim=[-0.05, 1.05], ylabel='True positive rate')\n    ymin, ymax = ax.get_ylim(); xmin, xmax = ax.get_xlim()\n    plt.vlines(x=fpr[thr_idx1], ymin=ymin, ymax=tpr[thr_idx1], color='k', alpha=.6, linestyle='--', axes=ax) # plot line for fpr at threshold\n    plt.hlines(y=tpr[thr_idx1], xmin=xmin, xmax=fpr[thr_idx1], color='k', alpha=.6, linestyle='--', axes=ax) # plot line for tpr at threshold\n    plt.vlines(x=fpr[thr_idx2], ymin=ymin, ymax=tpr[thr_idx2], color='k', alpha=.6, linestyle='--', axes=ax) # plot line for fpr at threshold\n    plt.hlines(y=tpr[thr_idx2], xmin=xmin, xmax=fpr[thr_idx2], color='k', alpha=.6,  linestyle='--', axes=ax) # plot line for tpr at threshold\n    plot_roc_curve(lasso_nonIMID, X_test+1, y_test, name = \"LASSO\", ax=ax, color='#D55E00', lw=2, alpha=.8)\n    plt.text(0.56,0.81, '1', fontsize='large')\n    plt.text(0.08,0.45, '2', fontsize='large')\n    sns.despine(fig=fig,ax=ax, trim=True)\n    fig.savefig('roc_pre-SLE_Rest.png', bbox_inches='tight', dpi=300, transparent=True)\n    fig.savefig('roc_pre-SLE_Rest.pdf', bbox_inches='tight', transparent=True)\n\n\n\n\n\nThreshold 0.6Threshold 0.7Threshold 0.8Threshold 0.84\n\n\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"]  + [\"dsDNA1\"])\neval_model(lasso_nonIMID, X_test+1, y_test, 'preSLE', 'rest_large', threshold=0.6)\n\nThreshold for classification: 0.6\n              precision    recall  f1-score   support\n\n  rest_large       0.99      0.67      0.80       462\n      preSLE       0.08      0.82      0.15        17\n\n    accuracy                           0.67       479\n   macro avg       0.54      0.75      0.48       479\nweighted avg       0.96      0.67      0.78       479\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. preSLE); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. preSLE); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\n\n\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lasso_nonIMID, X_test+1, y_test, 'preSLE', 'rest_large', threshold=0.7)\n\nThreshold for classification: 0.7\n              precision    recall  f1-score   support\n\n  rest_large       0.99      0.81      0.89       462\n      preSLE       0.12      0.71      0.20        17\n\n    accuracy                           0.80       479\n   macro avg       0.55      0.76      0.54       479\nweighted avg       0.96      0.80      0.86       479\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. preSLE); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. preSLE); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\n\n\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lasso_nonIMID, X_test+1, y_test, 'preSLE', 'rest_large', threshold=0.8)\n\nThreshold for classification: 0.8\n              precision    recall  f1-score   support\n\n  rest_large       0.98      0.91      0.95       462\n      preSLE       0.18      0.53      0.27        17\n\n    accuracy                           0.90       479\n   macro avg       0.58      0.72      0.61       479\nweighted avg       0.95      0.90      0.92       479\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. preSLE); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. preSLE); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\n\n\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lasso_nonIMID, X_test+1, y_test, 'preSLE', 'rest_large', threshold=0.84)\n\nThreshold for classification: 0.84\n              precision    recall  f1-score   support\n\n  rest_large       0.98      0.94      0.96       462\n      preSLE       0.25      0.53      0.34        17\n\n    accuracy                           0.93       479\n   macro avg       0.62      0.74      0.65       479\nweighted avg       0.96      0.93      0.94       479\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. preSLE); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. preSLE); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest on LLD (vs. rest)\n\nDefault thresholdThreshold for high specificity\n\n\n\nX_test, y_test = prep_data(X_test_df, 'LLD', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lasso_nonIMID, X_test+1, y_test, 'LLD', 'rest_large')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n  rest_large       0.97      0.51      0.67       462\n         LLD       0.08      0.71      0.15        28\n\n    accuracy                           0.52       490\n   macro avg       0.52      0.61      0.41       490\nweighted avg       0.92      0.52      0.64       490\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. LLD); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. LLD); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\nSensitivity was actually better for dsDNA only… But again, specificity here is much higher\n\n\n\nX_test, y_test = prep_data(X_test_df, 'LLD', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lasso_nonIMID, X_test+1, y_test, 'LLD', 'rest_large', threshold = 0.84)\n\nThreshold for classification: 0.84\n              precision    recall  f1-score   support\n\n  rest_large       0.95      0.94      0.95       462\n         LLD       0.21      0.25      0.23        28\n\n    accuracy                           0.90       490\n   macro avg       0.58      0.60      0.59       490\nweighted avg       0.91      0.90      0.91       490\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. LLD); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. LLD); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest on IMID (vs. blood bank)\n\nDefault thresholdThreshold for high specificity\n\n\n\nX_test, y_test = prep_data(data_all, 'IMID', 'BBD', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lasso_nonIMID, X_test+1, y_test, 'IMID', 'BBD')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n         BBD       0.62      0.58      0.60       361\n        IMID       0.59      0.63      0.61       346\n\n    accuracy                           0.60       707\n   macro avg       0.60      0.60      0.60       707\nweighted avg       0.60      0.60      0.60       707\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. IMID); specificity for the other group (BBD)\nN.B.: \"precision\" = PPV for the group in this row (e.g. IMID); NPV for the other group (BBD)\n\n\n\n\n\n\n\n\n\n\n\n\nX_test, y_test = prep_data(data_all, 'IMID', 'BBD', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lasso_nonIMID, X_test+1, y_test, 'IMID', 'BBD', threshold = 0.84)\n\nThreshold for classification: 0.84\n              precision    recall  f1-score   support\n\n         BBD       0.54      0.99      0.70       361\n        IMID       0.96      0.13      0.23       346\n\n    accuracy                           0.57       707\n   macro avg       0.75      0.56      0.47       707\nweighted avg       0.75      0.57      0.47       707\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. IMID); specificity for the other group (BBD)\nN.B.: \"precision\" = PPV for the group in this row (e.g. IMID); NPV for the other group (BBD)"
  },
  {
    "objectID": "notebooks/SLE Versus nonIMID.html#random-forest",
    "href": "notebooks/SLE Versus nonIMID.html#random-forest",
    "title": "Models for discriminating SLE patients from nonIMID controls",
    "section": "Random Forest",
    "text": "Random Forest\n\nclf_rf = RandomForestClassifier(random_state=40)\nnp.mean(cross_val_score(clf_rf, X_nonIMID, y_nonIMID, cv=cv, scoring = 'roc_auc'))\n\n0.8237835389830939\n\n\nUntuned RF with no preprocessing performs similarly to logistic regression (but a little bit worse). Also very similar to current XGBoost models"
  },
  {
    "objectID": "notebooks/projection.html",
    "href": "notebooks/projection.html",
    "title": "Dimensionality reduction / projections",
    "section": "",
    "text": "Citation\n\n\n\nThis notebook contains analyses for the following project:\nBrunekreef TE, Reteig LC, Limper M, Haitjema S, Dias J, Mathsson-Alm L, van Laar JM, Otten HG. Microarray analysis of autoantibodies can identify future Systemic Lupus Erythematosus patients. Human Immunology. 2022 Apr 11. doi:10.1016/j.humimm.2022.03.010"
  },
  {
    "objectID": "notebooks/projection.html#setup",
    "href": "notebooks/projection.html#setup",
    "title": "Dimensionality reduction / projections",
    "section": "Setup",
    "text": "Setup\n\nimport os\nfrom time import time\nimport feather\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.datasets import make_circles, load_digits\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nimport umap\nimport umap.plot\n\nfrom sle.modeling import generate_data\n%load_ext autoreload\n%autoreload 2\n\n\n\n\n\n\n\nRunning the code without the original data\n\n\n\nIf you want to run the code but don’t have access to the data, run the following instead to generate some synthetic data:\n\n\ndata_all = generate_data('imid')\nX_test_df = generate_data('rest')\n\n\nCode for loading original data\ndata_dir = os.path.join('..', 'data', 'processed')\ndata_all = feather.read_dataframe(os.path.join(data_dir, 'imid.feather'))\nX_test_df = feather.read_dataframe(os.path.join(data_dir,'rest.feather'))"
  },
  {
    "objectID": "notebooks/projection.html#tsne",
    "href": "notebooks/projection.html#tsne",
    "title": "Dimensionality reduction / projections",
    "section": "TSNE",
    "text": "TSNE\n\nToy data\nFrom https://scikit-learn.org/stable/auto_examples/manifold/plot_t_sne_perplexity.html#sphx-glr-auto-examples-manifold-plot-t-sne-perplexity-py\n\nn_samples = 300\nn_components = 2\nperplexities = [5, 30, 50, 100]\n\n\nX, y = make_circles(n_samples=n_samples, factor=.5, noise=.05)\n\nred = y == 0\ngreen = y == 1\n\n\n(fig, subplots) = plt.subplots(1, 5, figsize=(25, 5))\nax = subplots[0]\nax.scatter(X[red, 0], X[red, 1], c=\"r\")\nax.scatter(X[green, 0], X[green, 1], c=\"g\")\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nax.set_title(\"Original data\")\nplt.axis('tight')\n\nfor i, perplexity in enumerate(perplexities):\n    ax = subplots[i + 1]\n\n    t0 = time()\n    tsne = TSNE(n_components=n_components, init='random',\n                         random_state=0, perplexity=perplexity)\n    Y = tsne.fit_transform(X)\n    t1 = time()\n    print(\"circles, perplexity=%d in %.2g sec\" % (perplexity, t1 - t0))\n    ax.set_title(\"Perplexity=%d\" % perplexity)\n    ax.scatter(Y[red, 0], Y[red, 1], c=\"r\")\n    ax.scatter(Y[green, 0], Y[green, 1], c=\"g\")\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    ax.axis('tight')\n\ncircles, perplexity=5 in 0.88 sec\ncircles, perplexity=30 in 1.1 sec\ncircles, perplexity=50 in 0.95 sec\ncircles, perplexity=100 in 1.3 sec\n\n\n\n\n\n\n\nReal data\n\ndata_all.head()\n\n\n\n\n\n  \n    \n      \n      Actinin\n      ASCA\n      Beta2GP1\n      C1q\n      C3b\n      Cardiolipin\n      CCP1arg\n      CCP1cit\n      CENP\n      CMV\n      ...\n      SMP\n      TIF1gamma\n      TPO\n      tTG\n      Arthritis\n      Pleurisy\n      Pericarditis\n      Nefritis\n      dsDNA1\n      Class\n    \n  \n  \n    \n      0\n      94.8911\n      1117.530\n      1328.0800\n      88.0225\n      115.4250\n      55.1872\n      42.1010\n      40.3490\n      65.5990\n      1177.9000\n      ...\n      157.0750\n      283.8950\n      1011.080\n      170.611\n      0.0\n      0.0\n      0.0\n      0.0\n      11.0\n      SLE\n    \n    \n      1\n      99.9188\n      1295.260\n      119.1230\n      133.0480\n      59.4884\n      39.9630\n      39.0714\n      39.0714\n      35.5006\n      1023.6600\n      ...\n      114.7650\n      84.1488\n      1111.850\n      146.075\n      0.0\n      0.0\n      0.0\n      1.0\n      63.0\n      SLE\n    \n    \n      2\n      121.3530\n      2636.220\n      38.4903\n      85.9066\n      117.8180\n      38.4903\n      42.0952\n      40.2934\n      53.7753\n      76.1163\n      ...\n      113.3960\n      154.8510\n      109.857\n      128.418\n      1.0\n      0.0\n      1.0\n      0.0\n      2.6\n      SLE\n    \n    \n      3\n      145.0990\n      995.634\n      509.1220\n      171.8770\n      179.0070\n      60.6069\n      67.8459\n      52.4473\n      203.9280\n      8717.1000\n      ...\n      148.6730\n      4777.2800\n      765.190\n      211.928\n      1.0\n      0.0\n      0.0\n      1.0\n      1.6\n      SLE\n    \n    \n      4\n      66.0117\n      994.225\n      40.8654\n      184.3840\n      85.9921\n      44.1397\n      42.5038\n      43.3220\n      33.4571\n      3849.9300\n      ...\n      66.8153\n      103.4140\n      716.172\n      237.993\n      1.0\n      0.0\n      0.0\n      1.0\n      22.0\n      SLE\n    \n  \n\n5 rows × 63 columns\n\n\n\n\ndata_all.shape\n\n(1408, 63)\n\n\n\ndata_all['Class'].value_counts()\n\nSLE        483\nBBD        361\nIMID       346\nnonIMID    218\nName: Class, dtype: int64\n\n\n\nsymptoms = ['Arthritis', 'Pleurisy', 'Pericarditis', 'Nefritis']\nfor col in symptoms: # convert to boolean, so 0 or NaN become false\n    data_all[col] = data_all[col].fillna(False).astype('bool')\n\n\ndata_all['Symptoms'] = 'more_than_one' # new column for symptoms with default value for patients with more than one symptom\ndata_all.loc[data_all['Arthritis'] & ~data_all[['Pleurisy','Pericarditis','Nefritis']].any(1),'Symptoms'] = 'arthritis_only' # patients with only arthritis\ndata_all.loc[data_all['Nefritis'] & ~data_all[['Pleurisy','Pericarditis','Arthritis']].any(1),'Symptoms'] = 'nefritis_only' # patients with only nefritis\ndata_all.loc[data_all[['Pleurisy','Pericarditis']].any(1) & ~data_all[['Nefritis','Arthritis']].any(1),'Symptoms'] = 'pleurisy_or_pericarditis_only' # patients with either pleuritis or pericardits and nothing else\ndata_all.loc[~data_all[symptoms].any(1),'Symptoms'] = 'none' # patients with no symptoms\n\n\ndata_all.Symptoms.value_counts()\n\nnone                             1040\nnefritis_only                     108\nmore_than_one                     102\narthritis_only                    101\npleurisy_or_pericarditis_only      57\nName: Symptoms, dtype: int64\n\n\n\ndf = data_all.copy()\ny_class = data_all[\"Class\"]\ny_symptom = data_all['Symptoms']\nX = np.array(data_all.loc[:, ~data_all.columns.isin(symptoms + ['Class'] + ['Symptoms'] + ['dsDNA1'])])\n\ndf_symps = (sle .groupby(symptoms) .size() #.reset_index(name=‘counts’) #.sort_values(by=‘counts’,ascending=False) ) df_symps\n\nDefault parameters\n\ntsne = TSNE(random_state=0, verbose=1)\nY_tsne = tsne.fit_transform(X)\n\n[t-SNE] Computing 91 nearest neighbors...\n[t-SNE] Indexed 1408 samples in 0.014s...\n[t-SNE] Computed neighbors for 1408 samples in 0.230s...\n[t-SNE] Computed conditional probabilities for sample 1000 / 1408\n[t-SNE] Computed conditional probabilities for sample 1408 / 1408\n[t-SNE] Mean sigma: 1413.796264\n[t-SNE] KL divergence after 250 iterations with early exaggeration: 68.268234\n[t-SNE] KL divergence after 1000 iterations: 1.017738\n\n\n\n# add tsne results back to pandas dataframe\ndf['tsne_1'] = Y_tsne[:,0]\ndf['tsne_2'] = Y_tsne[:,1]\n\n\nplt.figure(figsize=(7,7))\nsns.scatterplot(\n    x=\"tsne_1\", y=\"tsne_2\",\n    hue=\"Class\",\n    data=df,\n    alpha=0.5)\nplt.axis('off')\n\n(-52.50039465973089, 40.55504019806097, -62.39752480691256, 49.14921090310397)\n\n\n\n\n\n\nThe blood bank controls (BBD) are the most prominent cluster\nSLE patients also seem to cluster together at the bottom\nIMIDs seem a bit closer to SLE than the nonIMIDs are\nThere could be a 2nd cluster smaller that is a mix of all 4 groups. Check if we also see this with different perplexities/UMAP\n\n\n\nDifferent perplexities\n\nperplexities = np.linspace(5,50,10)\n\n\nSLE = y_class.values == \"SLE\"\nIMID = y_class.values == \"IMID\"\nnonIMID = y_class.values == \"nonIMID\"\nBBD = y_class.values == \"BBD\"\n\n(fig, subplots) = plt.subplots(2, round(len(perplexities)/2), figsize=(25, 10))\ni = 0\nj = 0\nfor perplexity in perplexities:\n    if i == round(len(perplexities)/2):\n        i = 0\n        j = 1\n    ax = subplots[j][i]\n\n    t0 = time()\n    tsne = TSNE(random_state=0, perplexity=perplexity)\n    Y_tsne = tsne.fit_transform(X)\n    t1 = time()\n    print(\"perplexity=%d in %.2g sec\" % (perplexity, t1 - t0))\n    ax.set_title(\"Perplexity=%d\" % perplexity)\n    ax.scatter(Y_tsne[SLE, 0], Y_tsne[SLE, 1], c=\"r\")\n    ax.scatter(Y_tsne[IMID, 0], Y_tsne[IMID, 1], c=\"g\")\n    ax.scatter(Y_tsne[nonIMID, 0], Y_tsne[nonIMID, 1], c=\"b\")\n    ax.scatter(Y_tsne[BBD, 0], Y_tsne[BBD, 1], c=\"c\")\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    ax.axis('tight')\n    i += 1\n\nfig.legend(['SLE', 'IMID', 'non-IMID', 'Blood bank'])\n\nperplexity=5 in 3.2 sec\nperplexity=10 in 3.4 sec\nperplexity=15 in 3.8 sec\nperplexity=20 in 3.9 sec\nperplexity=25 in 4.2 sec\nperplexity=30 in 4.5 sec\nperplexity=35 in 4.6 sec\nperplexity=40 in 4.8 sec\nperplexity=45 in 4.7 sec\nperplexity=50 in 5.1 sec\n\n\n<matplotlib.legend.Legend at 0x7fee314a8950>\n\n\n\n\n\nDifferent perplexities don’t seem to change the overall picture too much"
  },
  {
    "objectID": "notebooks/projection.html#umap",
    "href": "notebooks/projection.html#umap",
    "title": "Dimensionality reduction / projections",
    "section": "UMAP",
    "text": "UMAP\n\nToy data\n\nDefault parameters\n\ndigits = load_digits()\n\n\nfig, ax_array = plt.subplots(20, 20)\naxes = ax_array.flatten()\nfor i, ax in enumerate(axes):\n    ax.imshow(digits.images[i], cmap='gray_r')\nplt.setp(axes, xticks=[], yticks=[], frame_on=False)\nplt.tight_layout(h_pad=0.5, w_pad=0.01)\n\n\n\n\n\nreducer = umap.UMAP(random_state=42)\n\n\nembedding = reducer.fit_transform(digits.data)\nembedding.shape\n\n(1797, 2)\n\n\n\nplt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('UMAP projection of the Digits dataset', fontsize=24);\n\n\n\n\n\n\nPlay with parameters\n\nnp.random.seed(42)\ndata = np.random.rand(800, 4) # sample from 4 dimensions\n\n\nfit = umap.UMAP()\n%time u = fit.fit_transform(data)\n\nCPU times: user 6.46 s, sys: 522 ms, total: 6.98 s\nWall time: 2.91 s\n\n\n\nplt.scatter(u[:,0], u[:,1], c=data) # plot 4D data as: R,G,B,alpha\nplt.title('UMAP embedding of random colours');\n\n\n\n\n\ndef draw_umap(n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean', title=''):\n    fit = umap.UMAP(\n        n_neighbors=n_neighbors,\n        min_dist=min_dist,\n        n_components=n_components,\n        metric=metric\n    )\n    u = fit.fit_transform(data);\n    fig = plt.figure()\n    if n_components == 1:\n        ax = fig.add_subplot(111)\n        ax.scatter(u[:,0], range(len(u)), c=data)\n    if n_components == 2:\n        ax = fig.add_subplot(111)\n        ax.scatter(u[:,0], u[:,1], c=data)\n    if n_components == 3:\n        ax = fig.add_subplot(111, projection='3d')\n        ax.scatter(u[:,0], u[:,1], u[:,2], c=data, s=100)\n    plt.title(title, fontsize=18)\n\n\nn_neighbors\n\nfor n in (2, 5, 10, 20, 50, 100, 200):\n    draw_umap(n_neighbors=n, title='n_neighbors = {}'.format(n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmin_dist\n\nfor d in (0.0, 0.1, 0.25, 0.5, 0.8, 0.99):\n    draw_umap(min_dist=d, title='min_dist = {}'.format(d))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReal data\n\nsc = StandardScaler()\ntrf = PowerTransformer(method='box-cox')\n\n\ndf_combined = pd.concat([data_all, rest[rest.Class.isin(['preSLE', 'rest_large'])]])\ndf_combined['Class'] = pd.Categorical(df_combined.Class, categories = ['SLE','IMID','nonIMID','rest_large','BBD','preSLE']) # order for plotting: largest groups first\ndf_combined.sort_values(by='Class', inplace=True)\nX_combined = np.array(df_combined.loc[:, ~df_combined.columns.isin(symptoms + ['Class'] + ['Symptoms'] + ['dsDNA1'])])\nX_combined_scaled = trf.fit_transform(X_combined+1)\n\n\nDefault parameters\n\numap_obj = umap.UMAP(random_state=42)\nY_umap = umap_obj.fit_transform(X)\n\n\nY_umap_combined = umap_obj.fit_transform(X_combined_scaled)\n\n\n# add umap results back to pandas dataframes\ndf['umap_1'] = Y_umap[:,0]\ndf['umap_2'] = Y_umap[:,1]\n\n\n# add umap results back to pandas dataframes\ndf_combined['umap_1'] = Y_umap_combined[:,0]\ndf_combined['umap_2'] = Y_umap_combined[:,1]\n\n\ndf_combined.Class.value_counts()\n\nSLE           483\nrest_large    462\nBBD           361\nIMID          346\nnonIMID       218\npreSLE         17\nName: Class, dtype: int64\n\n\n\nwith sns.plotting_context(\"paper\"):\n    sns.set(font=\"Arial\")\n    sns.set_style('white')\n    f, ax = plt.subplots(figsize=(5.5, 5.5))\n    g = sns.scatterplot(x='umap_1',y='umap_2',\n                    hue='Class', style='Class', size='Class',\n                    markers = {\"SLE\": \"o\", \"rest_large\": \"o\", \"BBD\": \"o\", \"IMID\": \"o\", \"nonIMID\": \"o\", \"preSLE\": \"o\"},\n                    sizes = {\"SLE\": 20, \"rest_large\": 20, \"BBD\": 20, \"IMID\": 20, \"nonIMID\": 20, \"preSLE\": 20},\n                    palette={\"SLE\": '#0072B2', \"rest_large\": '#D55E00', \"BBD\": '#CC79A7', \"IMID\": '#009E73', \"nonIMID\": '#E69F00', \"preSLE\": '#000000'},\n                    alpha=.7,\n                    data=df_combined,\n                    ax=ax)\n    legend = ax.legend(frameon=False, loc = 'upper left', bbox_to_anchor=(0,1.05)) # remove legend box; push legend up from upper left\n    ax.set_xlabel('Dimension 1'); ax.set_ylabel('Dimension 2'); \n    ax.set_ylim(-6,0)\n    new_labels = ['','SLE','IMID','Non-IMID','Rest','BBD','Pre-SLE']; # remove legend title and change group spelling\n    for t, l in zip(legend.texts, new_labels): t.set_text(l)\n    sns.despine(fig=f,ax=ax)\n    #f.savefig('umap.png', bbox_inches='tight', dpi=300, transparent=True)\n    #sf.savefig('umap.pdf', bbox_inches='tight', transparent=True)\n\n\n\n\n\ndef umap_vs_tsne(df,label):\n    plt.figure(figsize=(14,7))\n\n    ax1 = plt.subplot(1, 2, 1)\n    sns.scatterplot(\n        x=\"tsne_1\", y=\"tsne_2\",\n        hue=label,\n        data=df,\n        alpha=0.5,\n        legend=False,\n        ax=ax1)\n    ax1.set_title(\"t-SNE (default)\")\n    plt.axis('off')\n    ax2 = plt.subplot(1, 2, 2)\n    sns.scatterplot(\n        x=\"umap_1\", y=\"umap_2\",\n        hue=label,\n        data=df,\n        alpha=0.5,\n        ax=ax2)\n    ax2.set_title(\"UMAP (default)\")\n    plt.axis('off')\n\n\numap_vs_tsne(df,'Class')\n\n\n\n\nLooks fairly similar to t-SNE results. Clustering of blood bank controls is even clearer, while perhaps SLE is less clear\n\n# filter patients with no symptoms\numap_vs_tsne(df[df['Symptoms'].isin(['nefritis_only','arthritis_only','pleurisy_or_pericarditis_only', 'nefritis'])],'Symptoms')\n\n\n\n\n\nInteractive (with all points)\n\nfrom bokeh.plotting import show, save, output_notebook, output_file\noutput_notebook()\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\nhover_data = df[['Class','Symptoms']].reset_index()\n\n\nmapper = umap_obj.fit(X)\n\n\np = umap.plot.interactive(mapper, labels=y_class, hover_data=hover_data, point_size=4, theme='fire')\nshow(p)\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\np = umap.plot.interactive(mapper, labels=y_symptom, hover_data=hover_data, point_size=4, theme='fire')\nshow(p)\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nDifferent parameters\n\nn_neighborss = (2, 5, 10, 20, 50, 100, 200)\nmin_dists = (0.0, 0.1, 0.25, 0.5, 0.8, 0.99)\n\n\n(fig, subplots) = plt.subplots(len(min_dists), len(n_neighborss), figsize=(30,30))\n\nfor i, n_neighbors in enumerate(n_neighborss):\n    for j, min_dist in enumerate(min_dists):\n        ax = subplots[j][i]\n        u = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist).fit_transform(X);\n        df_params = pd.DataFrame({'umap_1': u[:,0], 'umap_2': u[:,1], 'Class': y_class.values})\n        sns.scatterplot(x='umap_1', y='umap_2', hue='Class', data=df_params, alpha=0.5, ax=ax, legend=False)\n        ax.set_title('n_neighbors = {}, min_dist = {}'.format(n_neighbors, min_dist))\n        ax.axis('off')\n\n/home/lcreteig/miniconda3/envs/SLE/lib/python3.7/site-packages/sklearn/manifold/_spectral_embedding.py:236: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n/home/lcreteig/miniconda3/envs/SLE/lib/python3.7/site-packages/sklearn/manifold/_spectral_embedding.py:236: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n/home/lcreteig/miniconda3/envs/SLE/lib/python3.7/site-packages/sklearn/manifold/_spectral_embedding.py:236: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n/home/lcreteig/miniconda3/envs/SLE/lib/python3.7/site-packages/sklearn/manifold/_spectral_embedding.py:236: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n/home/lcreteig/miniconda3/envs/SLE/lib/python3.7/site-packages/sklearn/manifold/_spectral_embedding.py:236: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n/home/lcreteig/miniconda3/envs/SLE/lib/python3.7/site-packages/sklearn/manifold/_spectral_embedding.py:236: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n\n\n\n\n\nOverall pattern remains the same. Could consider slightly higher than default parameters (for both) if we want a less “pinched” shape"
  },
  {
    "objectID": "notebooks/Main Results.html",
    "href": "notebooks/Main Results.html",
    "title": "Main Results",
    "section": "",
    "text": "Citation\n\n\n\nThis notebook reproduces results reported in the following paper:\nBrunekreef TE, Reteig LC, Limper M, Haitjema S, Dias J, Mathsson-Alm L, van Laar JM, Otten HG. Microarray analysis of autoantibodies can identify future Systemic Lupus Erythematosus patients. Human Immunology. 2022 Apr 11. doi:10.1016/j.humimm.2022.03.010"
  },
  {
    "objectID": "notebooks/Main Results.html#setup",
    "href": "notebooks/Main Results.html#setup",
    "title": "Main Results",
    "section": "Setup",
    "text": "Setup\n\nimport os\nimport feather\nimport pandas as pd\nimport numpy as np\nimport umap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, plot_roc_curve, roc_auc_score\n\nfrom sle.modeling import generate_data, prep_data, eval_model, calc_roc_cv, plot_roc_cv\nfrom sle.penalization import regularization_range, choose_C\n%load_ext autoreload\n%autoreload 2\n\n\n\n\n\n\n\nRunning the code without the original data\n\n\n\nIf you want to run the code but don’t have access to the data, run the following instead to generate some synthetic data:\n\n\ndata_all = generate_data('imid')\nX_test_df = generate_data('rest')\n\n\nCode for loading original data\ndata_dir = os.path.join('..', 'data', 'processed')\ndata_all = feather.read_dataframe(os.path.join(data_dir, 'imid.feather'))\nX_test_df = feather.read_dataframe(os.path.join(data_dir,'rest.feather'))"
  },
  {
    "objectID": "notebooks/Main Results.html#projection",
    "href": "notebooks/Main Results.html#projection",
    "title": "Main Results",
    "section": "Projection",
    "text": "Projection\nSee the projection.ipynb notebook for extended results and analyses\n\ntrf = PowerTransformer(method='box-cox')\n\n\ndf_combined = pd.concat([data_all, X_test_df[X_test_df.Class.isin(['preSLE', 'rest_large'])]])\ndf_combined['Class'] = pd.Categorical(df_combined.Class, categories = ['SLE','IMID','nonIMID','rest_large','BBD','preSLE']) # order for plotting: largest groups first\ndf_combined.sort_values(by='Class', inplace=True)\nX_combined = np.array(df_combined.loc[:, ~df_combined.columns.isin([\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + ['Class'] + ['dsDNA1'])])\nX_combined_scaled = trf.fit_transform(X_combined+1)\n\n\nX_combined.shape\n\n(1887, 57)\n\n\n\ndf_combined.Class.value_counts()\n\nSLE           483\nrest_large    462\nBBD           361\nIMID          346\nnonIMID       218\npreSLE         17\nName: Class, dtype: int64\n\n\n\numap_obj = umap.UMAP(random_state=42)\nY_umap_combined = umap_obj.fit_transform(X_combined_scaled)\n\n\ndf_combined['umap_1'] = Y_umap_combined[:,0]\ndf_combined['umap_2'] = Y_umap_combined[:,1]\n\n\nFigure 1\n\nwith sns.plotting_context(\"paper\"):\n    sns.set(font=\"Arial\")\n    sns.set_style('white')\n    f, ax = plt.subplots(figsize=(5.5, 5.5))\n    g = sns.scatterplot(x='umap_1',y='umap_2',\n                    hue='Class', style='Class', size='Class',\n                    markers = {\"SLE\": \"o\", \"rest_large\": \"o\", \"BBD\": \"o\", \"IMID\": \"o\", \"nonIMID\": \"o\", \"preSLE\": \"o\"},\n                    sizes = {\"SLE\": 20, \"rest_large\": 20, \"BBD\": 20, \"IMID\": 20, \"nonIMID\": 20, \"preSLE\": 20},\n                    palette={\"SLE\": '#0072B2', \"rest_large\": '#D55E00', \"BBD\": '#CC79A7', \"IMID\": '#009E73', \"nonIMID\": '#E69F00', \"preSLE\": '#000000'},\n                    alpha=.7,\n                    data=df_combined,\n                    ax=ax)\n    legend = ax.legend(frameon=False, loc = 'upper left', bbox_to_anchor=(0,1.05))\n    ax.set_xlabel('Dimension 1'); ax.set_ylabel('Dimension 2'); \n    ax.set_ylim(-6,0)\n    new_labels = ['','SLE','IMID','Non-IMID','Rest','BBD','Pre-SLE']; # remove legend title and change group spelling\n    for t, l in zip(legend.texts, new_labels): t.set_text(l)\n    sns.despine(fig=f,ax=ax)"
  },
  {
    "objectID": "notebooks/Main Results.html#prediction-models",
    "href": "notebooks/Main Results.html#prediction-models",
    "title": "Main Results",
    "section": "Prediction models",
    "text": "Prediction models\n\nX_nonIMID, y_nonIMID = prep_data(data_all, 'SLE', 'nonIMID', drop_cols = [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\",\"dsDNA1\"])\ndsDNA_nonIMID = X_nonIMID.dsDNA2.values.reshape(-1,1)\n\n\nclf = LogisticRegression(penalty = 'none', max_iter = 10000)\n\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=40)\n\n\nSLE vs. non-IMID\nSee the SLE Versus nonIMID.ipynb notebook for extended analyses and results\n\nLogistic regression: Only dsDNA from microarray\n\nlr_dsDNA_nonIMID = clf.fit(dsDNA_nonIMID, y_nonIMID)\n\n\nnp.mean(cross_val_score(clf, dsDNA_nonIMID, y_nonIMID, cv=cv, scoring = 'roc_auc'))\n\n0.8006217102395325\n\n\n\n\nLogistic regression: Whole microarray\n\nXp1_nonIMID = X_nonIMID + 1 # Some < 0 values > -1. Because negative fluorescence isn't possible, and Box-Cox requires strictly positive values, add ofset\n\n\npipe_trf = Pipeline([\n        ('transform', trf),\n        ('clf', clf)])\n\n\nnp.mean(cross_val_score(pipe_trf, Xp1_nonIMID, y_nonIMID, cv=cv, scoring = 'roc_auc'))\n\n0.8337294463757692\n\n\n\n\nLASSO\n\nclf_lasso = LogisticRegression(penalty='l1', max_iter = 10000, solver = 'liblinear')\n\n\nK = 100\nlambda_min, lambda_max = regularization_range(Xp1_nonIMID,y_nonIMID,trf)\nCs_lasso_nonIMID = np.logspace(np.log10(1/lambda_min),np.log10(1/lambda_max), K)\npipe = Pipeline([\n        ('trf', trf),\n        ('clf', clf_lasso)\n])\nparams = [{\n    \"clf__C\": Cs_lasso_nonIMID\n}]\n\nlasso_nonIMID = GridSearchCV(pipe, params, cv = cv, scoring = 'roc_auc', refit=choose_C)\n\n\n%%time\nlasso_nonIMID.fit(Xp1_nonIMID,y_nonIMID)\n\nCPU times: user 3min 33s, sys: 64.1 ms, total: 3min 33s\nWall time: 3min 33s\n\n\nGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=40),\n             estimator=Pipeline(steps=[('trf',\n                                        PowerTransformer(method='box-cox')),\n                                       ('clf',\n                                        LogisticRegression(max_iter=10000,\n                                                           penalty='l1',\n                                                           solver='liblinear'))]),\n             param_grid=[{'clf__C': array([6.52012573, 6.08069108, 5.67087285, 5.288675  , 4.932236  ,\n       4.5998198 , 4.28980735, 4.00068869, 3.73105566, 3...\n       0.04932236, 0.0459982 , 0.04289807, 0.04000689, 0.03731056,\n       0.03479595, 0.03245082, 0.03026374, 0.02822407, 0.02632186,\n       0.02454785, 0.02289341, 0.02135047, 0.01991152, 0.01856955,\n       0.01731803, 0.01615085, 0.01506234, 0.01404719, 0.01310045,\n       0.01221753, 0.0113941 , 0.01062618, 0.00991001, 0.00924211,\n       0.00861922, 0.00803832, 0.00749656, 0.00699132, 0.00652013])}],\n             refit=<function choose_C at 0x7f80d43e74d0>, scoring='roc_auc')\n\n\nBest model AUC:\n\nlasso_nonIMID.cv_results_['mean_test_score'].max()\n\n0.8373932541974528\n\n\nAUC with lambda selected through 1 SE rule:\n\nlasso_nonIMID.cv_results_['mean_test_score'][lasso_nonIMID.best_index_]\n\n0.8334140652811983\n\n\nNumber of non-zero coefficients in this model:\n\ncoefs_final_nonimid = (pd.Series(lasso_nonIMID.best_estimator_.named_steps.clf.coef_.squeeze(), \n                                 index = X_nonIMID.columns)[lambda x: x!=0].sort_values())\nlen(coefs_final_nonimid)\n\n29\n\n\n\n\nFigure 2 (top panel)\n\nsns.reset_defaults()\nwith sns.plotting_context(\"paper\"):\n    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n    tprs, aucs = calc_roc_cv(lasso_nonIMID.best_estimator_,cv,Xp1_nonIMID,y_nonIMID)\n    fig, ax = plot_roc_cv(tprs, aucs, fig, ax, fig_title='SLE vs. non-IMID', line_color='#E69F00', legend_label='LASSO')\n    tprs, aucs = calc_roc_cv(lr_dsDNA_nonIMID,cv,dsDNA_nonIMID,y_nonIMID)\n    fig, ax = plot_roc_cv(tprs, aucs, fig, ax, reuse=True, line_color='gray', legend_label='dsDNA (from chip) only')\n    sns.despine(fig=fig,ax=ax, trim=True)\n    plt.legend(frameon=False)\n    plt.show()\n\n\n\n\n\n\nSupplemental Figure 1a\n\nsns.reset_defaults()\npal = sns.diverging_palette(250, 15, s=75, l=40, center=\"dark\", as_cmap=True)\nwith sns.plotting_context(\"paper\"):\n    with sns.axes_style(\"whitegrid\"):\n        fig, ax = plt.subplots(figsize=(2.5, 5))\n        g = sns.scatterplot(y=coefs_final_nonimid.index, x=coefs_final_nonimid.values, \n                            hue=coefs_final_nonimid.values, palette=pal, legend=False)\n        ax.set(title='SLE vs.non-IMID',\n               xticks=np.linspace(-1.0,1.0,5), xlim=[-1.2,1.2], xlabel=r'$\\beta$')\n        sns.despine(fig=fig,ax=ax, left=True, bottom=True)\n\n\n\n\n\n\n\nSLE vs. BBD (Blood Bank Donors)\nSee the SLE Versus BBD.ipynb notebook for extended analyses and results\n\nLogistic regression: Only one feature\n\nX, y = prep_data(data_all, 'SLE', 'BBD', drop_cols = [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\ndsDNA = X.dsDNA2.values.reshape(-1,1)\nRo60 = X.Ro60.values.reshape(-1,1)\n\n\ndsDNA from microarray\n\nlr_dsDNA = clf.fit(dsDNA, y)\n\n\nnp.mean(cross_val_score(clf, dsDNA, y, cv=cv, scoring = 'roc_auc'))\n\n0.7956906204723647\n\n\n\n\nRo60 from microarray\n\nnp.mean(cross_val_score(clf, Ro60, y, cv=cv, scoring = 'roc_auc'))\n\n0.8980197798817389\n\n\n\n\n\nLASSO\n\nXp1 = X + 1 # Some values are between -1 and 0. Because negative fluorescence isn't possible, and Box-Cox requires strictly positive values, add ofset\n\n\nlambda_min, lambda_max = regularization_range(Xp1,y,trf)\n\n\nK = 100\nCs_lasso = np.logspace(np.log10(1/lambda_min),np.log10(1/lambda_max), K)\npipe = Pipeline([\n        ('trf', trf),\n        ('clf', clf_lasso)\n])\nparams = [{\n    \"clf__C\": Cs_lasso\n}]\n\nsearch_lasso = GridSearchCV(pipe, params, cv = cv, scoring = 'roc_auc', refit=choose_C)\n\n\n%%time\nsearch_lasso.fit(Xp1,y)\n\nCPU times: user 9min 2s, sys: 17min 14s, total: 26min 16s\nWall time: 4min 36s\n\n\nGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=40),\n             estimator=Pipeline(steps=[('trf',\n                                        PowerTransformer(method='box-cox')),\n                                       ('clf',\n                                        LogisticRegression(max_iter=10000,\n                                                           penalty='l1',\n                                                           solver='liblinear'))]),\n             param_grid=[{'clf__C': array([3.42342671, 3.19269921, 2.97752197, 2.77684695, 2.58969676,\n       2.41515987, 2.25238618, 2.10058289, 1.95901...\n       0.02589697, 0.0241516 , 0.02252386, 0.02100583, 0.01959011,\n       0.0182698 , 0.01703848, 0.01589014, 0.0148192 , 0.01382043,\n       0.01288898, 0.01202031, 0.01121018, 0.01045465, 0.00975004,\n       0.00909292, 0.00848009, 0.00790856, 0.00737555, 0.00687846,\n       0.00641488, 0.00598254, 0.00557933, 0.0052033 , 0.00485262,\n       0.00452557, 0.00422056, 0.00393611, 0.00367083, 0.00342343])}],\n             refit=<function choose_C at 0x7f80d43e74d0>, scoring='roc_auc')\n\n\nBest model AUC:\n\nsearch_lasso.cv_results_['mean_test_score'].max()\n\n0.9829972773711079\n\n\nAUC with lambda selected through 1 SE rule:\n\nsearch_lasso.cv_results_['mean_test_score'][search_lasso.best_index_]\n\n0.9813334155499589\n\n\nNumber of non-zero coefficients in this model:\n\ncoefs_final = (pd.Series(search_lasso.best_estimator_.named_steps.clf.coef_.squeeze(), \n                                 index = X_nonIMID.columns)[lambda x: x!=0].sort_values())\nlen(coefs_final)\n\n15\n\n\n\n\nFigure 2 (bottom panel)\n\nsns.reset_defaults()\nwith sns.plotting_context(\"paper\"):\n    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n    tprs, aucs = calc_roc_cv(search_lasso.best_estimator_,cv,Xp1,y)\n    fig, ax = plot_roc_cv(tprs, aucs, fig, ax, fig_title='SLE vs. BBD', line_color='#CC79A7', legend_label='LASSO')\n    tprs, aucs = calc_roc_cv(lr_dsDNA,cv,dsDNA,y)\n    fig, ax = plot_roc_cv(tprs, aucs, fig, ax, reuse=True, line_color='gray', legend_label='dsDNA only')\n    sns.despine(fig=fig,ax=ax, trim=True)\n    plt.legend(frameon=False)\n    plt.show()\n\n\n\n\n\n\nSupplemental Figure 1b\n\nsns.reset_defaults()\npal = sns.diverging_palette(250, 15, s=75, l=40, center=\"dark\", as_cmap=True)\nwith sns.plotting_context(\"paper\"):\n    with sns.axes_style(\"whitegrid\"):\n        fig, ax = plt.subplots(figsize=(2.5, 2.5))\n        g = sns.scatterplot(y=coefs_final.index, x=coefs_final.values, \n                            hue=coefs_final.values, palette=pal, legend=False)\n        ax.set(title='SLE vs. BBD',\n               xticks=np.linspace(-1.0,1.0,5), xlim=[-1.2,1.2], xlabel=r'$\\beta$')\n        sns.despine(fig=fig,ax=ax, left=True, bottom=True)"
  },
  {
    "objectID": "notebooks/Main Results.html#prediction-of-the-development-of-sle",
    "href": "notebooks/Main Results.html#prediction-of-the-development-of-sle",
    "title": "Main Results",
    "section": "Prediction of the development of SLE",
    "text": "Prediction of the development of SLE\nROC AUC of SLE vs. BBD model\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\nroc_auc_score(y_test, search_lasso.predict_proba(X_test+1)[:,1])\n\n0.5679908326967151\n\n\n\nFigure 3\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\n# ROC curve\nthreshold1=0.5\nthreshold2=0.84\nfpr, tpr, thresholds = roc_curve(y_test, lasso_nonIMID.predict_proba(X_test+1)[:,1])\nthr_idx1 = (np.abs(thresholds - threshold1)).argmin() # find index of value closest to chosen threshold\nthr_idx2 = (np.abs(thresholds - threshold2)).argmin() # find index of value closest to chosen threshold\nsns.reset_defaults()\nwith sns.plotting_context(\"paper\"):\n    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n    ax.plot([0, 1], [0, 1], linestyle='--', lw=1.5, color='k', alpha=.25)\n    ax.set(title=\"Pre-SLE vs. Rest\",\n           xlim=[-0.05, 1.05], xlabel='False positive rate', \n           ylim=[-0.05, 1.05], ylabel='True positive rate')\n    ymin, ymax = ax.get_ylim(); xmin, xmax = ax.get_xlim()\n    plt.vlines(x=fpr[thr_idx1], ymin=ymin, ymax=tpr[thr_idx1], color='k', alpha=.6, linestyle='--', axes=ax) # plot line for fpr at threshold\n    plt.hlines(y=tpr[thr_idx1], xmin=xmin, xmax=fpr[thr_idx1], color='k', alpha=.6, linestyle='--', axes=ax) # plot line for tpr at threshold\n    plt.vlines(x=fpr[thr_idx2], ymin=ymin, ymax=tpr[thr_idx2], color='k', alpha=.6, linestyle='--', axes=ax) # plot line for fpr at threshold\n    plt.hlines(y=tpr[thr_idx2], xmin=xmin, xmax=fpr[thr_idx2], color='k', alpha=.6,  linestyle='--', axes=ax) # plot line for tpr at threshold\n    plot_roc_curve(lasso_nonIMID, X_test+1, y_test, name = \"LASSO\", ax=ax, color='#D55E00', lw=2, alpha=.8)\n    plt.text(0.56,0.81, '1', fontsize='large')\n    plt.text(0.08,0.45, '2', fontsize='large')\n    sns.despine(fig=fig,ax=ax, trim=True)\n\n\n\n\n\n\nClassification\n\npre-SLE vs. rest, default thresholdpre-SLE vs. rest, high-specificity threshold\n\n\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lasso_nonIMID, X_test+1, y_test, 'preSLE', 'rest_large')\n\nThreshold for classification: 0.5\n              precision    recall  f1-score   support\n\n  rest_large       0.99      0.51      0.67       462\n      preSLE       0.06      0.88      0.12        17\n\n    accuracy                           0.52       479\n   macro avg       0.53      0.70      0.39       479\nweighted avg       0.96      0.52      0.65       479\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. preSLE); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. preSLE); NPV for the other group (rest_large)\n\n\n\n\n\n\n\n\n\n\n\n\nX_test, y_test = prep_data(X_test_df, 'preSLE', 'rest_large', [\"Arthritis\",\"Pleurisy\",\"Pericarditis\",\"Nefritis\"] + [\"dsDNA1\"])\neval_model(lasso_nonIMID, X_test+1, y_test, 'preSLE', 'rest_large', threshold=0.84)\n\nThreshold for classification: 0.84\n              precision    recall  f1-score   support\n\n  rest_large       0.98      0.94      0.96       462\n      preSLE       0.25      0.53      0.34        17\n\n    accuracy                           0.93       479\n   macro avg       0.62      0.74      0.65       479\nweighted avg       0.96      0.93      0.94       479\n\nN.B.: \"recall\" = sensitivity for the group in this row (e.g. preSLE); specificity for the other group (rest_large)\nN.B.: \"precision\" = PPV for the group in this row (e.g. preSLE); NPV for the other group (rest_large)"
  }
]